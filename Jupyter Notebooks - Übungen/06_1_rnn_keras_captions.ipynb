{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyF9u-gpi4o1"
      },
      "source": [
        "# Language Modeling with Recurrent Neural Networks using Keras\n",
        "checked 28.02.24 GPaa√ü\n",
        "\n",
        "This notebook uses code from [here](http://www.cs.virginia.edu/~vicente/vislang/notebooks/language_generation_lab.html). It uses [Keras](https://keras.io/), a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of  [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model.\n",
        "\n",
        "Prediction Task: **Language Modelling**\n",
        "* predict next words in a text given a history of previous words.\n",
        "* Dataset: A set of 400000 captions for images.\n",
        "\n",
        "This model can be used to compute the probability of a sequence, as well as generate new sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8t5JVA3i4o2"
      },
      "outputs": [],
      "source": [
        "import os, sys;\n",
        "from matplotlib import pyplot\n",
        "%matplotlib inline\n",
        "\n",
        "import glob\n",
        "import numpy as np\n",
        "import os, sys\n",
        "import math\n",
        "import json\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`print_mat`: pretty-print a matrix or dataframe"
      ],
      "metadata": {
        "id": "myiJzGifBpFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def print_mat(x, title=\"\", prtDim=True, max_rows=10, max_columns=10, precision=3, doRound=True,index=None, rowNames=None, colNames=None ):\n",
        "    \"\"\" use pandas display to print a dataframe\n",
        "        title: to be printed\n",
        "        max_rows: number or None\n",
        "        max_columns: number or None\n",
        "        precision: number\n",
        "        doRound: True  perform rounding (avoid E notation)\n",
        "        index: None  row names\n",
        "        columns: None column names\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    with pd.option_context('display.max_rows', max_rows, 'display.max_columns', max_columns, 'display.precision',precision):\n",
        "        # pd.options.display.max_columns = None\n",
        "        if tf.is_tensor(x):\n",
        "            x = x.numpy()\n",
        "        if doRound:\n",
        "            x = np.round(x,decimals=precision)\n",
        "        if title!=\"\":\n",
        "            if prtDim:\n",
        "                print(title,x.shape)\n",
        "            else:\n",
        "                print(title,x.shape)\n",
        "        display(pd.DataFrame(x,index=rowNames, columns=colNames))     # use smaller font\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yHX_sbZBBkWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM3UtsCki4o3"
      },
      "source": [
        "## Dataset of Image Captions\n",
        "\n",
        "We will first read the sentences from the ms-coco dataset. This file was downloaded from http://mscoco.org/dataset/#download. This file contains ~5 descriptions for 80,000 images for a total of ~400k descriptions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyR_9wyfi4o3"
      },
      "source": [
        "## Reading and Preprocessing\n",
        "Each word is translated to a numerical index.\n",
        "When we apply the model to generation later, it will output words as indices, so we'll need to map each numerical index back to its corresponding string representation. We'll reverse the lexicon dictionary so that a word can be looked up by its index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At2_jW_Di4o3"
      },
      "outputs": [],
      "source": [
        "modelType=\"big\"\n",
        "if modelType==\"small\":\n",
        "    use_perc = 30\n",
        "elif modelType==\"big\":\n",
        "    use_perc = 100      # only read this percentage of the data\n",
        "else:\n",
        "    raise TypeError(\"only small or big\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip"
      ],
      "metadata": {
        "id": "Vl4nQZxiuCtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip annotations_trainval2014.zip"
      ],
      "metadata": {
        "id": "YWQI0803uKM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "s1QgBZsJi4o3"
      },
      "outputs": [],
      "source": [
        "vocabularySize = 1000  # vocabulary size.\n",
        "assert(0 < use_perc and use_perc <= 100)\n",
        "mscoco = json.load(open('annotations/captions_train2014.json'))\n",
        "#captionStrings = ['[START] ' + entry['caption'].encode('ascii') for entry in mscoco['annotations']]\n",
        "\n",
        "captionStrings = []\n",
        "for entry in mscoco['annotations']:\n",
        "    if 'caption' in entry and len(str(entry['caption']))>0:\n",
        "        captionStrings.append(str(entry['caption']))\n",
        "print('Number of sentences', len(captionStrings))\n",
        "lng = math.floor(len(captionStrings)*use_perc*0.01)\n",
        "\n",
        "print('use_perc',use_perc)\n",
        "captionStrings = captionStrings[:lng]\n",
        "print('Kept number of sentences', len(captionStrings))\n",
        "print('First 2 sentences in the list:\\n', captionStrings[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqcLnhLqi4o4"
      },
      "source": [
        "## Definining a word vocabulary\n",
        "Next, we define a vocabulary and assign each unique word in this dataset with a word id. We use the 1000 most common words in these captions. Then we can transform each sentence into an array of word ids. These preprocessing functionalities are already implemented in keras Tokenizer class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQUW279ei4o4"
      },
      "outputs": [],
      "source": [
        "# Split sentences into words, and define a vocabulary with the most common words.\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words = vocabularySize,\n",
        "    filters = '!\"#$%&()*+,-./:;<=>?@\\\\^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(captionStrings)\n",
        "\n",
        "# Convert the sentences into sequences of word ids using our vocabulary.\n",
        "captionSequences = tokenizer.texts_to_sequences(captionStrings)\n",
        "\n",
        "# Keep dictionaries that map ids -> words, and words -> ids.\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {idx: word for (word, idx) in word2id.items()}\n",
        "maxSeqLen = max(\n",
        "    [len(seq)\n",
        "     for seq in captionSequences])  # Find the sentence with most words.\n",
        "\n",
        "\n",
        "print('Max Sequence Length', maxSeqLen)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some output to verify the above.\n",
        "for i in range(2):\n",
        "  print('Original string:\\t', captionStrings[i])\n",
        "  print('Sequence of Word Ids:\\t', captionSequences[i])\n",
        "  print('Word Ids back to Words:\\t',\n",
        "      \" \".join([id2word[idx] for idx in captionSequences[i]]))"
      ],
      "metadata": {
        "id": "6NSWXvmBBHnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iMsdTDci4o4"
      },
      "source": [
        "## Padding to Maximum Length\n",
        "Another piece of pre-processing that we might need is padding the sequences with zeroes so that all sequences have the same length and we can put them in a single matrix. This is implemented in Keras using the pad_sequences function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDeRsHKdi4o4"
      },
      "outputs": [],
      "source": [
        "# By default it pads with zeroes at the beginning (why would that be preferrable?), but we are overriding\n",
        "# that default behavior by using padding = 'post'.\n",
        "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
        "padded_seqs = pad_sequences(\n",
        "    captionSequences,\n",
        "    maxlen=(maxSeqLen + 1),\n",
        "    padding='post',\n",
        "    truncating='post')\n",
        "\n",
        "id2word[0] = 'END'  # id2word[0] is empty before\n",
        "word2id['END'] = 0\n",
        "\n",
        "# Let's print some output.\n",
        "print(padded_seqs.shape)  # This is num_sentences x maxSeqLen.\n",
        "# Let's try converting back the first sequence into words again.\n",
        "print(\" \".join([id2word[idx] for idx in padded_seqs[0]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU31K7qhi4o4"
      },
      "source": [
        "The outputs to be predicted are shifted for one position"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrVzeQ4-i4o4"
      },
      "outputs": [],
      "source": [
        "inputData = padded_seqs[:, :-1]  # words 1, 2, 3, ... , (n-1)\n",
        "outputData = padded_seqs[:, 1:]  # words 2, 3, 4, ... , (n)\n",
        "print_mat(inputData,\"inputData\",doRound=False,max_rows=5,max_columns=None)\n",
        "print_mat(outputData,\"outputData\",doRound=False,max_rows=5,max_columns=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fnrgrE8i4o4"
      },
      "source": [
        "Create Training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umHfjM0li4o4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    inputData, outputData, test_size=0.20, random_state=42)\n",
        "print(\"x_train.shape\",x_train.shape,\"y_train.shape\",y_train.shape)\n",
        "print(\"x_test.shape\",x_test.shape,\"y_test.shape\",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TYBDK7vi4o4"
      },
      "source": [
        "## Building our model using a Recurrent Neural Network\n",
        "\n",
        "Next we will create a recurrent neural network using Keras.\n",
        "- It takes an input set of words of size `(batch_size, maxSeqLen)`,\n",
        "- The output of this network will be a vector of size `(batch_size, maxSeqLen, vocabularySize)`. <br>\n",
        "Notice that the output is of a different size than the input, it contains a pseudo-probability distribution (the output of a softmax layer) for every time step in the sequence. Meaning, it outputs the probability for each word in the vocabulary to be the next word at each time step.\n",
        "<img src=\"img/RNN.png\",style=\"max-width:70%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uL1GI-gi4o4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "if modelType==\"small\":  # for model with 30% data\n",
        "    emb_size = 128\n",
        "    hid_size = 256\n",
        "    dropout = 0.3\n",
        "    batch_size=256\n",
        "else:                  # for model with 100% data\n",
        "    emb_size = 256    # original: 300\n",
        "    hid_size = 512    # original: 512\n",
        "    dropout = 0.3\n",
        "    emb_size = 128    # original: 300\n",
        "    hid_size = 256    # original: 512\n",
        "    dropout = 0.3\n",
        "    batch_size=256\n",
        "rnnType = 'lstm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8whZEE4Ui4o4"
      },
      "outputs": [],
      "source": [
        "def create_model(rnnType, maxSeqLen, vocabularySize, emb_size,hid_size,dropout):\n",
        "    if rnnType == 'rnn': RNN = layers.SimpleRNN\n",
        "    if rnnType == 'gru': RNN = layers.GRU\n",
        "    if rnnType == 'lstm': RNN = layers.LSTM\n",
        "\n",
        "    print('Building training model...')\n",
        "    # Remember that in libraries like Keras/Tensorflow, you only need to implement the forward pass.\n",
        "    # Here we show how to do that for our model.\n",
        "\n",
        "    # Define the shape of the inputs batchSize x (maxSeqLen + 1).\n",
        "    words = keras.Input(batch_shape=(None, maxSeqLen),\n",
        "                       name=\"input\")\n",
        "\n",
        "    # Build a matrix of size vocabularySize x 300 where each row corresponds to a \"word embedding\" vector.\n",
        "    # This layer will convert replace each word-id with a word-vector of size 300.\n",
        "    embeddings = layers.Embedding(\n",
        "        vocabularySize, emb_size, name=\"embeddings\")(words)\n",
        "\n",
        "    # Pass the word-vectors to the LSTM layer.\n",
        "    # We are setting the hidden-state size to 512.\n",
        "    # The output will be batchSize x maxSeqLen x hiddenStateSize\n",
        "    hiddenStates = RNN(\n",
        "        hid_size,\n",
        "        return_sequences=True,  # return hidden vector for each position, not only the final\n",
        "        input_shape=(maxSeqLen, emb_size),\n",
        "        dropout=dropout,        # use dropout for regularization\n",
        "        name=\"rnn\")(embeddings)\n",
        "\n",
        "    # Apply a linear (Dense) layer of size 512 x 256 to the outputs of the LSTM at each time step.\n",
        "    denseOutput = layers.TimeDistributed(\n",
        "        layers.Dense(vocabularySize), name=\"linear\")(hiddenStates)\n",
        "    # generate probabilities for words\n",
        "    predictions = layers.TimeDistributed(\n",
        "        layers.Activation(\"softmax\"), name=\"softmax\")(denseOutput)\n",
        "\n",
        "    # Build the computational graph by specifying the input, and output of the network.\n",
        "    model = keras.Model(inputs=words, outputs=predictions)\n",
        "\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(rnnType, maxSeqLen, vocabularySize, emb_size,hid_size,dropout)\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy'],\n",
        "    optimizer=keras.optimizers.RMSprop(lr=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ5blQGNi4o4"
      },
      "source": [
        "Sample 10 inputs from the training data and verify everything works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WUfjFaVi4o4"
      },
      "outputs": [],
      "source": [
        "sample_inputs = padded_seqs[0:10, :-1]    # exclude last element\n",
        "sample_outputs = model.predict(sample_inputs)\n",
        "print('input size', sample_inputs.shape)\n",
        "print('predictes output size', sample_outputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vcHR2kLi4o5"
      },
      "outputs": [],
      "source": [
        "sample_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhHsUif3i4o5"
      },
      "source": [
        "\n",
        "## Training the Model\n",
        "\n",
        "Keras already implements a generic trainModel functionality through the model.fit function, but it also contains model.train_on_batch which we might need to save memory (e.g. if we want to avoid loading all the dataset in memory at once). For more informations about Keras model functionalities you can see here: https://keras.io/models/model/\n",
        "\n",
        "If you installed Tensorflow with GPU support, this will automatically run on the GPU!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnunNIO6i4o5"
      },
      "outputs": [],
      "source": [
        "modelSavePathSmall = os.getcwd()+'/small_'+rnnType + \"_best.keras\"\n",
        "modelSavePathBig = os.getcwd()+'/big_'+rnnType + \"_best.keras\"\n",
        "if modelType==\"small\":  # for model with 30% data\n",
        "    epochs = 5\n",
        "    modelSavePath = modelSavePathSmall\n",
        "else:\n",
        "    epochs = 15\n",
        "    modelSavePath = modelSavePathBig\n",
        "\n",
        "# create output directory\n",
        "print(\"Will save best model to\",modelSavePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uok_8c2Gi4o5"
      },
      "source": [
        "To save the best model only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue8RkaWli4o5"
      },
      "outputs": [],
      "source": [
        "ModelCheckpoint = tf.keras.callbacks.ModelCheckpoint\n",
        "checkpointer = ModelCheckpoint(\n",
        "    filepath=modelSavePath,\n",
        "    save_best_only=True,\n",
        "    monitor='loss')\n",
        "# configure early stopping\n",
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                      patience=3)  # Stop after this number of epochs with no improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-fnptULi4o5"
      },
      "source": [
        "V100: ~ 22 sec/epoch if 100% of data is used (emb_size = 300 ,hid_size = 512 )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG0Q41npi4o5"
      },
      "outputs": [],
      "source": [
        "epochs=1  #21 sec/epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "msSx8Yq-i4o5"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    validation_data = (x_test,y_test),\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpointer,estop])\n",
        "\n",
        "!ls\n",
        "\n",
        "#model.save_weights(modelSavePath)\n",
        "#print(\"Saved parameters to\",modelSavePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJ1WcVuxi4o5"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.save(modelSavePath, save_format ='keras')\n",
        "print(\"Saved parameters to\",modelSavePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMfegMF9i4o5"
      },
      "source": [
        "100 % sample\n",
        "```\n",
        "Epoch 36/50\n",
        "331290/331290 [==============================] - 23s 70us/sample - loss: 0.5747 - accuracy: 0.8751 - val_loss: 0.5744 - val_accuracy: 0.8758\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZbRS81Vi4o5"
      },
      "source": [
        "**Monitoring** <br>\n",
        "You can check CPU activity with `htop` in a terminal. <br>\n",
        "You can check GPU activity with `watch nvidia-smi` in a terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g4AA7qni4o5"
      },
      "source": [
        "### Explicit  Training Loop\n",
        "We could also go batch by batch ourselves, however the above\n",
        "function worked well so let's not go this way.\n",
        "```\n",
        "trainSize = inputData.shape[0]\n",
        "batchSize = 100\n",
        "nBatches =  trainSize / batchSize\n",
        "for b in range(0, nBatches):\n",
        "    # Build the batch inputs, and batch labels.\n",
        "    batchInputs = np.zeros((batchSize, inputData.shape[1]))\n",
        "    batchLabels = np.zeros((batchSize, inputData.shape[1], vocabularySize))\n",
        "    for bi in range(0, batchSize):\n",
        "        rand_int = random.randint(0, trainSize - 1)\n",
        "        batchInputs[bi, :] = inputData[rand_int, :]\n",
        "        for s in range(0, inputData.shape[1]):\n",
        "            batchLabels[bi, s, outputData[rand_int, s]] = 1\n",
        "\n",
        "     model.train_on_batch(batchInputs, batchLabels)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_tuzW0di4o5"
      },
      "outputs": [],
      "source": [
        "# plot history\n",
        "pyplot.plot(history.history['loss'], label='train loss')\n",
        "pyplot.plot(history.history['val_loss'], label='validation loss')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzhvL5t0i4o5"
      },
      "source": [
        "\n",
        "## Building the Inference Model.\n",
        "\n",
        "Now let's build a model here with the exact same details as the ones we used for training,\n",
        "* however this one **only takes a single word**, and outputs the next word.\n",
        "* The other modification is that this network will keep the state of the recurrent network unless we override it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0JzivIwi4o5"
      },
      "outputs": [],
      "source": [
        "!ls # list files in output directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLuf6vZmi4o5"
      },
      "outputs": [],
      "source": [
        "readBigModel = True\n",
        "if readBigModel:\n",
        "    inference_model = keras.models.load_model(modelSavePathBig)\n",
        "else:\n",
        "    inference_model = keras.models.load_model(modelSavePathSmall)\n",
        "inference_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItbKyNUqi4o5"
      },
      "outputs": [],
      "source": [
        "shortSeqLen=1  # only 1 word as start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDzrNauVi4o6"
      },
      "source": [
        "Given the token 'a' predict the next most likely word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4sjGNggi4o6"
      },
      "outputs": [],
      "source": [
        "#startWord = np.zeros((1, 1))\n",
        "startWord = np.zeros((1, maxSeqLen))\n",
        "startWord[0, 0] = word2id['the']\n",
        "nextWordProbabilities = inference_model.predict(startWord)\n",
        "\n",
        "# print the most probable words that goes next.\n",
        "top_inds = (-nextWordProbabilities).argsort()[0, 0, :10]\n",
        "top_probs = np.sort(-nextWordProbabilities)[0, 0, :10]\n",
        "\n",
        "# Print the next probable word given the previous word.\n",
        "for iw in top_inds:\n",
        "    print(\"{:10.3f}\".format(nextWordProbabilities[0,0,iw]), id2word[iw])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CG1CvDpci4o6"
      },
      "source": [
        "## Sampling a Complete New Sentence\n",
        "\n",
        "Now that we have our inference_model working we can start producing new sentences by random sampling from the output of next word probabilities one step at a time. We rely on the np.random.multinomial function from numpy. To see what it does please check the documentation and make sure you understand what it does http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_CG5AiKi4o6"
      },
      "outputs": [],
      "source": [
        "inference_model.reset_states()  # This makes sure the initial hidden state is cleared every time.\n",
        "word1 = 'the'\n",
        "startWord = np.zeros((1, maxSeqLen))\n",
        "startWord[0, 0] = word2id[word1]\n",
        "print(word1,\"         = given first word\")\n",
        "for i in range(0, maxSeqLen):\n",
        "    nextWordProbs = inference_model.predict(startWord)[0,0,:]\n",
        "    nextWordProbs.shape\n",
        "    nextWordProbs = np.asarray(nextWordProbs).astype('float64')\n",
        "    nextWordProbs = nextWordProbs / nextWordProbs.sum()\n",
        "    nextWordId = np.random.multinomial(1, nextWordProbs.squeeze(), 1).argmax()\n",
        "    print(\"{:10.3f}\".format(nextWordProbs[nextWordId]), id2word[nextWordId],) # The comma at the end avoids printing a return line character.\n",
        "    startWord.fill(0)\n",
        "    startWord[0, 0] = nextWordId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBqbLutQi4o6"
      },
      "outputs": [],
      "source": [
        "# access the parameters of the model\n",
        "for ww in model.weights:\n",
        "    print(ww.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIR7Ofq5i4o6"
      },
      "outputs": [],
      "source": [
        "model.weights[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRyDI85Di4o6"
      },
      "source": [
        "Notice how the model learns to always predict 'END' once it has already predicted the first 'END' and does not produce any other word after that. We can stop the for loop once we already found 'END', this has the effect of producing sentences of arbitrary size, meaning our model has learned when to finish a sentence. The sentence might not be perfect at this point in training but probably it has already learned to produce basic sentences, however it still produces incoherent stuff from time to time. If you keep training the model for longer it should get better and better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ugc-TMIi4o6"
      },
      "source": [
        "## tf.data for fast Transfer of Data\n",
        "GPUs and TPUs can radically reduce the time required to execute a single training step. Achieving peak performance requires an efficient input pipeline that delivers data for the next step before the current step has finished. The tf.data API helps to build flexible and efficient input pipelines.\n",
        "\n",
        "The tf.data API introduces a tf.data.Dataset abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\n",
        "\n",
        "We have the following functionalities:\n",
        "* [`tf.data.Dataset.from_tensors()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors) to construct a `Dataset` from data in memory.<br/>\n",
        "`tf.data.TFRecordDataset()` to construct a `Dataset` from a file in the recommended `TFRecord` format.\n",
        "* [`Dataset.cache()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)  caches elements either in the specified file or in memory.\n",
        "* [`Dataset.shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) randomly shuffles the elements of this dataset..\n",
        "* [`Dataset.map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) apply per-element transformations.\n",
        "* [`Dataset.batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) to create batches.\n",
        "* [`Dataset.repeat(count=?)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) to repeats this dataset so each original value is seen count times (default: indefinite repetition).\n",
        "\n",
        "See the [documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for  a complete list of transformations.\n",
        "\n",
        "Let's now use [tf.data](https://www.tensorflow.org/api_docs/python/tf/data) to shuffle, batch, and cache the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK6qrHAyi4o6"
      },
      "outputs": [],
      "source": [
        "def pdata(nam, dat,n=2):\n",
        "    \"\"\" to print data from a tf.Dataset\"\"\"\n",
        "    print(nam)\n",
        "    itm =0\n",
        "    for elem in dat:\n",
        "        print(elem)\n",
        "        itm +=1\n",
        "        if itm>=n:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWghHnQni4o6"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "pdata(\"from_slices\",train_data,n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nXbku6wi4o6"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "train_data = train_data.cache()\n",
        "train_data = train_data.shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.batch(batch_size)\n",
        "pdata(\"\\nbatch with BATCH_SIZE=\"+str(batch_size),train_data,n=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIbK7LyFi4o6"
      },
      "outputs": [],
      "source": [
        "# Repeats this dataset so each original value is seen count times.\n",
        "train_data = train_data.repeat()\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_data = test_data.batch(batch_size).repeat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHnK2Bxzi4o6"
      },
      "outputs": [],
      "source": [
        "model1 = create_model(rnnType, maxSeqLen, vocabularySize, emb_size,hid_size,dropout)\n",
        "\n",
        "model1.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics = ['accuracy'],\n",
        "    optimizer=keras.optimizers.RMSprop(lr=0.001))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5nL5WDHi4o6"
      },
      "source": [
        "20% data: 23 sec/epoch\n",
        "~ 116 sec/epoch if 100% of data is used (emb_size = 300 ,hid_size = 512 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSZpSmQvi4o6"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = int(len(x_train)/batch_size)\n",
        "history = model1.fit(\n",
        "    train_data,\n",
        "    steps_per_epoch = steps_per_epoch,\n",
        "    validation_data = test_data,\n",
        "    validation_steps = 50,\n",
        "    epochs=epochs,\n",
        "    callbacks=[checkpointer])\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}