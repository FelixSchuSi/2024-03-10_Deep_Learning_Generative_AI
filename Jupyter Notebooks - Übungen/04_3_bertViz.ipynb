{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik2_B9NNZXmr"
      },
      "source": [
        "# Visualizing BERT Attentions\n",
        "BertViz from this [repository](https://github.com/jessevig/bertviz)\n",
        "\n",
        "checked 27.02.2024 GPaass\n",
        "\n",
        "extend by"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertviz"
      ],
      "metadata": {
        "id": "epQOQENLZzOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "9mp1x2kBZXmw"
      },
      "outputs": [],
      "source": [
        "# tag: parameters for papermill. View > Cell Toolbar > Tags. Need papermill library\n",
        "prm = \"small\"              # small: just use 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YovWHsr9ZXmz"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "# insert at 1, 0 is the script path (or '' in REPL)\n",
        "#sys.path.insert(1, '../bertviz-master')\n",
        "#os.listdir('../bertviz-master')\n",
        "\n",
        "from bertviz import head_view\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPILDbuVZXm0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.config.list_physical_devices())\n",
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjyq_6SyZXm1"
      },
      "outputs": [],
      "source": [
        "# clear GPU memory\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS-Bgs8NZXm2"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CShIWERSZXm2"
      },
      "source": [
        "## Predicting Masked Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VufDp2F-ZXm2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZL1CG9YZXm4"
      },
      "source": [
        "Pipelines are made of:\n",
        "- A :doc:`tokenizer <tokenizer>` in charge of mapping raw textual input to token.\n",
        "- A :doc:`model <model>` to make predictions from the inputs.\n",
        "- Some (optional) post processing for enhancing model's output.\n",
        "\n",
        "The first task argument determines the task:\n",
        "* `\"fill-mask\"`: will return a :class:`transformers.FillMaskPipeline`:. <br>\n",
        "    Masked language modeling prediction pipeline. This pipeline only works for inputs with exactly one token masked.\n",
        "\n",
        "### Predict English Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWfHhlHOZXm5"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"fill-mask\", model=\"bert-base-uncased\", top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4buYLv09ZXm6"
      },
      "outputs": [],
      "source": [
        "nlp(f\"This is the best thing I've {nlp.tokenizer.mask_token} in my life.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKO_pyYUZXm8"
      },
      "source": [
        "### What does Pipeline do?\n",
        "Before the model is trained a **tokenizer** ist estimated from the training data.\n",
        "\n",
        "Steps for model application.\n",
        "\n",
        "* tokenize the input to a sequence of integers\n",
        "* apply the model to the input\n",
        "* extract the ouput and decode the predicted tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ07YsLqZXm9"
      },
      "outputs": [],
      "source": [
        "inputs = f\"This is the best thing I've {nlp.tokenizer.mask_token} in my life.\"\n",
        "\n",
        "model_inputs=nlp.tokenizer(inputs, return_tensors=nlp.framework)  # apply the tokenizer\n",
        "print(model_inputs)\n",
        "nlp.tokenizer.decode(model_inputs['input_ids'].numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VTPas3QZXm9"
      },
      "outputs": [],
      "source": [
        "model_outputs=nlp.model(**model_inputs)                 # apply the model\n",
        "print(\"predictions for each token\")\n",
        "print(model_outputs,\"\\n\")\n",
        "model_outputs[\"input_ids\"] = model_inputs[\"input_ids\"]\n",
        "input_ids = model_outputs[\"input_ids\"][0]\n",
        "outputs = model_outputs[\"logits\"]\n",
        "                                                        # position of the mask\n",
        "masked_index = torch.nonzero(input_ids == nlp.tokenizer.mask_token_id, as_tuple=False).squeeze(-1)\n",
        "logits = outputs[0, masked_index, :]\n",
        "print(\"logits.shape\",logits.shape)\n",
        "probs = logits.softmax(dim=-1)                          # compute the probabilities\n",
        "values, predictions = probs.topk(5)                     # get top k probabilities and indices\n",
        "values = values.detach().numpy()[0]\n",
        "predictions = predictions.detach().numpy()[0]\n",
        "for i in range(len(values)):\n",
        "    print(values[i],\"\\t\",nlp.tokenizer.decode(predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_res(res, nlp):\n",
        "  for i in range(10):\n",
        "    r =res[i]\n",
        "    print(\"{:6.4f}\".format(r['score']),'\\t',r['token'],'\\t',nlp.tokenizer.decode(r['token']))"
      ],
      "metadata": {
        "id": "RyPgohf30U29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjV8X21zZXm_"
      },
      "outputs": [],
      "source": [
        "print_res(nlp(f\"The man went to the {nlp.tokenizer.mask_token} to buy some food.\"), nlp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laoOptf3bHbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R514MS-PZXnA"
      },
      "source": [
        "### Predict German Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feqt9n6RZXnB"
      },
      "outputs": [],
      "source": [
        "gnlp = pipeline(\"fill-mask\", model=\"bert-base-german-cased\", top_k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcnUTwjGZXnC"
      },
      "outputs": [],
      "source": [
        "print_res(gnlp(f\"Dies ist das Beste, was ich je in meinem Leben {nlp.tokenizer.mask_token} getan habe.\"), gnlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_IOAPIoZXnD"
      },
      "outputs": [],
      "source": [
        "print_res(gnlp(f\"Ich gehe zur {nlp.tokenizer.mask_token} , um Geld abzuheben.\"), gnlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVORFM8fZXnD"
      },
      "outputs": [],
      "source": [
        "print_res(gnlp(f\"Ich gehe zur {nlp.tokenizer.mask_token} und setze mich.\"), gnlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEWCPrBJZXnD"
      },
      "source": [
        "## Show Attention Strength\n",
        "### Show Attention Strength for All  Heads\n",
        "[notebook](https://github.com/jessevig/bertviz/blob/master/notebooks/model_view_bert.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wOXBSS7ZXnE"
      },
      "outputs": [],
      "source": [
        "from bertviz import model_view\n",
        "from bertviz.neuron_view import show\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TESGBty0ZXnE"
      },
      "outputs": [],
      "source": [
        "def show_model_view(model, tokenizer, sentence_a, sentence_b=None, hide_delimiter_attn=False, display_mode=\"dark\"):\n",
        "    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    if sentence_b:\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "        sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "    else:\n",
        "        attention = model(input_ids)[-1]\n",
        "        sentence_b_start = None\n",
        "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    if hide_delimiter_attn:\n",
        "        for i, t in enumerate(tokens):\n",
        "            if t in (\"[SEP]\", \"[CLS]\"):\n",
        "                for layer_attn in attention:\n",
        "                    layer_attn[0, :, i, :] = 0\n",
        "                    layer_attn[0, :, :, i] = 0\n",
        "    model_view(attention, tokens, sentence_b_start, display_mode=display_mode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0H89pO4vZXnE"
      },
      "outputs": [],
      "source": [
        "model_version = 'bert-base-uncased'\n",
        "do_lower_case = True\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"The cat sat on the mat\"\n",
        "sentence_b = \"The cat lay on the rug\"\n",
        "#sentence_a = \"I go to the bank to get money\"\n",
        "#sentence_b = \"I go to the bank of the river\"\n",
        "show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode=\"dark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrKkVgnLZXnF"
      },
      "source": [
        "### Show Attention Strength for a Single Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlEsGvekZXnF"
      },
      "outputs": [],
      "source": [
        "def show_head_view(model, tokenizer, sentence_a, sentence_b=None, layer=None, heads=None):\n",
        "    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=True)\n",
        "    input_ids = inputs['input_ids']\n",
        "    if sentence_b:\n",
        "        token_type_ids = inputs['token_type_ids']\n",
        "        attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
        "        sentence_b_start = token_type_ids[0].tolist().index(1)\n",
        "    else:\n",
        "        attention = model(input_ids)[-1]\n",
        "        sentence_b_start = None\n",
        "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)\n",
        "    head_view(attention, tokens, sentence_b_start, layer=layer, heads=heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbM6QmgoZXnF"
      },
      "outputs": [],
      "source": [
        "model_version = 'bert-base-uncased'\n",
        "do_lower_case = True\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"the rabbit quickly hopped\"\n",
        "sentence_b = \"The turtle slowly crawled\"\n",
        "show_head_view(model, tokenizer, sentence_a, sentence_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qWK1BKXQZXnF"
      },
      "outputs": [],
      "source": [
        "model_version = 'bert-base-uncased'\n",
        "do_lower_case = True\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"The boy met the girl\"\n",
        "sentence_b = \"She looked very pretty\"\n",
        "show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode=\"dark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W_4BvczZXnG"
      },
      "source": [
        "### Show Attention Strength for German Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "76etwsc2ZXnH"
      },
      "outputs": [],
      "source": [
        "model_version = 'bert-base-german-cased'\n",
        "do_lower_case = False\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"Der Junge traf das Mädchen\"\n",
        "sentence_b = \"Sie sah sehr schön aus\"\n",
        "show_model_view(model, tokenizer, sentence_a, sentence_b, hide_delimiter_attn=False, display_mode=\"dark\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVYAVMmqZXnH"
      },
      "source": [
        "The nex visualization shows the association to different parts of the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdhzlR7nZXnH"
      },
      "outputs": [],
      "source": [
        "model_version = 'bert-base-german-cased'\n",
        "do_lower_case = False\n",
        "model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
        "sentence_a = \"Der Junge traf das Mädchen\"\n",
        "sentence_b = \"Sie sah sehr schön aus\"\n",
        "show_head_view(model, tokenizer, sentence_a, sentence_b)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}