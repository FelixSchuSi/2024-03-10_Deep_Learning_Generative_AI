{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8UCwLc63K-N"
      },
      "source": [
        "Checked 27.10.2023 G.Paaß\n",
        "# Time Series Analysis with RNN using tf.data\n",
        "\n",
        "This notebook uses code from [here](https://www.tensorflow.org/tutorials/structured_data/time_series). It uses Keras, which is now part of Tensorflow.\n",
        "\n",
        "Prediction Task: **Time Series Analysis**\n",
        "* predict next value of a one-dimensional or multidimensional time series.\n",
        "* Dataset: A set of 400000 captions for images.\n",
        "\n",
        "This model can be used to compute the probability of a sequence, as well as generate new sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omy2KXU-3K-N"
      },
      "outputs": [],
      "source": [
        "import sys, os;\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os, sys\n",
        "import math\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkyvP5pG3K-O"
      },
      "source": [
        "## Wheather Dataset\n",
        "\n",
        "This tutorial uses a [weather time series dataset](https://www.bgc-jena.mpg.de/wetter/) recorded by the Max Planck Institute for Biogeochemistry.\n",
        "\n",
        "This dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. For efficiency, you will use only the data collected between 2009 and 2016. This section of the dataset was prepared by François Chollet for his book Deep Learning with Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDmCYSWE3K-P"
      },
      "outputs": [],
      "source": [
        "# download 'jena_climate_2009_2016.csv.zip' to directory '~/.keras/datasets/''\n",
        "zip_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
        "    fname='jena_climate_2009_2016.csv.zip',\n",
        "    extract=True)\n",
        "csv_path, _ = os.path.splitext(zip_path) # remove extension '.zip' from path\n",
        "csv_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvkOJuvY3K-P"
      },
      "outputs": [],
      "source": [
        "os.path.splitext(zip_path)    # remove suffix from path\n",
        "df = pd.read_csv(csv_path)    # Read a comma-separated values (csv) file into DataFrame.\n",
        "df.head()                     # print with pandas-formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqUFJhis3K-P"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the actual **variables** of the data:\n",
        "\n",
        "1. Date Time\n",
        "\n",
        "1. p (mbar) atmospheric pressure\n",
        "\n",
        "1. T (degC) temperature\n",
        "\n",
        "1. Tpot (K) potential temperature\n",
        "\n",
        "1. Tdew (degC) dew point temperature\n",
        "\n",
        "1. rh (%) relative humidity\n",
        "\n",
        "1. VPmax (mbar) saturation water vapor pressure\n",
        "\n",
        "1. VPact (mbar) actual water vapor pressure\n",
        "\n",
        "1. VPdef (mbar) water vapor pressure deficit\n",
        "\n",
        "1. sh (g/kg) specific humidity\n",
        "\n",
        "1. H2OC (mmol/mol) water vapor concentration\n",
        "\n",
        "1. rho (g/m3) air density\n",
        "\n",
        "1. wv (m/s) wind velocity\n",
        "\n",
        "1. max. wv (m/s) maximum wind velocity\n",
        "\n",
        "1. wd (deg) wind direction\n",
        "\n",
        "As you can see above, an observation is recorded every 10 minutes. This means that, for a single hour, you will have 6 observations. Similarly, a single day will contain 144 (6x24) observations.\n",
        "\n",
        "This tutorial will just deal with **hourly predictions**, so start by sub-sampling the data from 10-minute intervals to one-hour intervals:"
      ],
      "metadata": {
        "id": "G8UU5ruyYtMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(csv_path)\n",
        "# Slice [start:stop:step], starting from index 5 take every 6th record.\n",
        "df = df[5::6]\n",
        "\n",
        "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')"
      ],
      "metadata": {
        "id": "SPmAAUknZq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the evolution of a few features over time:\n",
        "* over the whole data period\n",
        "* over the first 480 measurements"
      ],
      "metadata": {
        "id": "Kqo36mh2Ztxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n",
        "plot_features = df[plot_cols]\n",
        "plot_features.index = date_time\n",
        "_ = plot_features.plot(subplots=True)\n",
        "\n",
        "plot_features = df[plot_cols][:480]\n",
        "plot_features.index = date_time[:480]\n",
        "_ = plot_features.plot(subplots=True)"
      ],
      "metadata": {
        "id": "HYtSwuC1ZIjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Data Statistics"
      ],
      "metadata": {
        "id": "HvibAijSZ_h7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().transpose()"
      ],
      "metadata": {
        "id": "Rmn0Hg3aaHWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **correlation matrix** shows that some of the variables are highly correlated."
      ],
      "metadata": {
        "id": "dA1CFvpFaM5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr,\n",
        "    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "    vmin=-1.0, vmax=1.0,\n",
        "    square=True, ax=ax)"
      ],
      "metadata": {
        "id": "izykcFH-aMQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wind velocity\n",
        "\n",
        "One thing that should stand out is the min value of the wind velocity (wv (m/s)) and the maximum value (max. wv (m/s)) columns. This -9999 is likely erroneous.\n",
        "\n",
        "There's a separate wind direction column, so the velocity should be greater than zero (>=0). Replace it with zeros:"
      ],
      "metadata": {
        "id": "--nxwEN0abVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = df['wv (m/s)']\n",
        "bad_wv = wv == -9999.0\n",
        "wv[bad_wv] = 0.0\n",
        "\n",
        "max_wv = df['max. wv (m/s)']\n",
        "bad_max_wv = max_wv == -9999.0\n",
        "max_wv[bad_max_wv] = 0.0\n",
        "\n",
        "# The above inplace edits are reflected in the DataFrame.\n",
        "df['wv (m/s)'].min()"
      ],
      "metadata": {
        "id": "CEeMy-hKalZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wind Variable\n",
        "\n",
        "The last column of the data, `wd (deg)`—gives the wind direction in units of degrees. Angles do not make good model inputs: 360° and 0° should be close to each other and wrap around smoothly. Direction shouldn't matter if the wind is not blowing.\n",
        "\n",
        "But this will be easier for the model to interpret if you convert the wind direction and velocity columns to a wind **vector**:\n",
        "\n"
      ],
      "metadata": {
        "id": "Z30I8ajrarlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wv = df.pop('wv (m/s)')\n",
        "max_wv = df.pop('max. wv (m/s)')\n",
        "\n",
        "# Convert to radians.\n",
        "wd_rad = df.pop('wd (deg)')*np.pi / 180\n",
        "\n",
        "# Calculate the wind x and y components.\n",
        "df['Wx'] = wv*np.cos(wd_rad)\n",
        "df['Wy'] = wv*np.sin(wd_rad)\n",
        "\n",
        "# Calculate the max wind x and y components.\n",
        "df['max Wx'] = max_wv*np.cos(wd_rad)\n",
        "df['max Wy'] = max_wv*np.sin(wd_rad)"
      ],
      "metadata": {
        "id": "SlHLx_DOa1R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of wind vectors is much simpler for the model to correctly interpret:"
      ],
      "metadata": {
        "id": "PfOr64ArbSKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist2d(df['Wx'], df['Wy'], bins=(50, 50), vmax=400)\n",
        "plt.colorbar()\n",
        "plt.xlabel('Wind X [m/s]')\n",
        "plt.ylabel('Wind Y [m/s]')\n",
        "ax = plt.gca()\n",
        "ax.axis('tight')"
      ],
      "metadata": {
        "id": "TCdzzLdDbWiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time\n",
        "\n",
        "Similarly, the `Date Time` column is very useful, but not in this string form. Start by converting it to seconds:"
      ],
      "metadata": {
        "id": "EZd_lClTbamE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_s = date_time.map(pd.Timestamp.timestamp)"
      ],
      "metadata": {
        "id": "AllL3hq3biSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the wind direction, the time in seconds is not a useful model input. Being weather data, it has clear daily and yearly periodicity. There are many ways you could deal with periodicity.\n",
        "\n",
        "You can get usable signals by using sine and cosine transforms to clear \"Time of day\" and \"Time of year\" signals:"
      ],
      "metadata": {
        "id": "wj76n_n0boA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "day = 24*60*60           # number of secods of a day\n",
        "year = (365.2425)*day    # number of seconds of a year\n",
        "\n",
        "df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
        "df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
        "df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
        "df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))"
      ],
      "metadata": {
        "id": "P_M_npMjbpGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.array(df['Day sin'])[:25])\n",
        "plt.plot(np.array(df['Day cos'])[:25])\n",
        "plt.xlabel('Time [h]')\n",
        "plt.title('Time of day signal')"
      ],
      "metadata": {
        "id": "s7B_pBafbtzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This gives the model access to the most important frequency features. In this case you knew ahead of time which frequencies were important.\n",
        "\n",
        "If you don't have that information, you can determine which frequencies are important by extracting features with <a href=\"https://en.wikipedia.org/wiki/Fast_Fourier_transform\" class=\"external\">Fast Fourier Transform</a>. To check the assumptions, here is the `tf.signal.rfft` of the temperature over time. Note the **obvious peaks at frequencies near `1/year` and `1/day`**:"
      ],
      "metadata": {
        "id": "MQqt9WY-bz_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fft = tf.signal.rfft(df['T (degC)'])\n",
        "f_per_dataset = np.arange(0, len(fft))\n",
        "\n",
        "n_samples_h = len(df['T (degC)'])\n",
        "hours_per_year = 24*365.2524\n",
        "years_per_dataset = n_samples_h/(hours_per_year)\n",
        "\n",
        "f_per_year = f_per_dataset/years_per_dataset\n",
        "plt.step(f_per_year, np.abs(fft))\n",
        "plt.xscale('log')\n",
        "plt.ylim(0, 400000)\n",
        "plt.xlim([0.1, max(plt.xlim())])\n",
        "plt.xticks([1, 365.2524], labels=['1/Year', '1/day'])\n",
        "_ = plt.xlabel('Frequency (log scale)')"
      ],
      "metadata": {
        "id": "EHcHw4pVb33d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen in the **correlation plot** windspeed, direction, and time is now correlated with the other variables."
      ],
      "metadata": {
        "id": "GbUtmA1MfWDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr,\n",
        "    cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
        "    vmin=-1.0, vmax=1.0,\n",
        "    square=True, ax=ax)"
      ],
      "metadata": {
        "id": "4Yx-PDbgfQhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q6s3y0ZSfUqG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsN3EDyz3K-P"
      },
      "source": [
        "\n",
        "\n",
        "### Defining  time series lag\n",
        "Given a specific time, let's say you want to predict the temperature 6 hours in the future. In order to make this prediction, you choose to use 5 days of observations. Thus, you would create a **window** containing the last 720(5x144) observations to train the model. Many such configurations are possible, making this dataset a good one to experiment with.\n",
        "\n",
        "The first 300,000 rows of the data will be the training dataset, and there remaining will be the validation dataset. This amounts to ~2100 days worth of training data.\n",
        "\n",
        "Setting seed to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyz7bh8g3K-P"
      },
      "outputs": [],
      "source": [
        "print(\"df.shape\",df.shape)\n",
        "TRAIN_SPLIT = round(df.shape[0]*0.8)   # first 80% are used as training data\n",
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 10000\n",
        "EVALUATION_INTERVAL = 200\n",
        "EPOCHS = 20\n",
        "tf.random.set_seed(13)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`plot_losses(history, title, ymax=-1)` and `plot_metric`"
      ],
      "metadata": {
        "id": "s3hd_fgi4JyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6gx7ebD3K-P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def plot_losses(history, title, ymax=-1):\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    #val_loss = np.array(val_loss)*uni_train_std # expand to degree C\n",
        "    #loss = np.array(loss)*uni_train_std\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss value\")\n",
        "    if ymax>0:\n",
        "        plt.ylim(0.0,ymax)  # limit y-range\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_metric(history, title, ymax=-1, metric=\"mean_absolute_error\", ylabel=\"mean absolute error in degree C\"):\n",
        "    metr = history.history[metric]\n",
        "    val_metr= history.history['val_'+metric]\n",
        "\n",
        "    #val_loss = np.array(val_loss)*uni_train_std # expand to degree C\n",
        "    #loss = np.array(loss)*uni_train_std\n",
        "\n",
        "    epochs = range(len(metr))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(ylabel)\n",
        "    if ymax>0:\n",
        "        plt.ylim(0.0,ymax)  # limit y-range\n",
        "\n",
        "    plt.plot(epochs, metr, 'b', label='Training '+metric)\n",
        "    plt.plot(epochs, val_metr, 'r', label='Validation '+metric)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv-kTPjk3K-P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def create_time_steps(length):\n",
        "    return list(range(-length, 0))\n",
        "\n",
        "def show_plot(plot_data, delta, title):\n",
        "    labels = ['History', 'True Future', 'Model Prediction']\n",
        "    marker = ['.-', 'rx', 'go']\n",
        "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "    if delta:\n",
        "        future = delta\n",
        "    else:\n",
        "        future = 0\n",
        "\n",
        "    plt.title(title)\n",
        "    for i, x in enumerate(plot_data):\n",
        "        if i:\n",
        "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "               label=labels[i])\n",
        "        else:\n",
        "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "    plt.legend()\n",
        "    plt.xlim([time_steps[0], (future+5)*2])\n",
        "    plt.xlabel('Time-Step')\n",
        "    return plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvPYAAey3K-P"
      },
      "source": [
        "## Forecasting Multivariate Time Series\n",
        "### Defining Input Features\n",
        "The original dataset contains fourteen features. For simplicity, this section considers only three of the original fourteen. The features used are air temperature, atmospheric pressure, and air density.\n",
        "\n",
        "To use more features, add their names to this list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttk4CSOE3K-P"
      },
      "source": [
        "`'Date Time', 'p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)',\n",
        "       'rh (%)', 'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)',\n",
        "       'H2OC (mmol/mol)', 'rho (g/m**3)', 'wv (m/s)', 'max. wv (m/s)',\n",
        "       'wd (deg)'`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "zfwo9YNCdjLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgPYrR2U3K-P"
      },
      "outputs": [],
      "source": [
        "features_considered = ['p (mbar)', 'T (degC)', 'Tpot (K)', 'Tdew (degC)', 'rh (%)',\n",
        "                        'VPmax (mbar)', 'VPact (mbar)', 'VPdef (mbar)', 'sh (g/kg)',\n",
        "                        'H2OC (mmol/mol)', 'rho (g/m**3)', 'Wx', 'Wy', 'max Wx', 'max Wy',\n",
        "                        'Day sin', 'Day cos', 'Year sin', 'Year cos']\n",
        "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']\n",
        "#features_considered = ['T (degC)', 'T (degC)']                  # only 1 variable\n",
        "\n",
        "name2id = {}\n",
        "for i in range(len(features_considered)):\n",
        "  name2id[features_considered[i]] = i\n",
        "print(\"name2id = \",name2id,\"\\n\")\n",
        "\n",
        "featDat = df[features_considered]\n",
        "#featDat.index = df['Date Time']\n",
        "featDat.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name2id"
      ],
      "metadata": {
        "id": "nET8J5-3g31z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yeytx8pC3K-P"
      },
      "source": [
        "#### Plotting Input Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7grATJO43K-P"
      },
      "outputs": [],
      "source": [
        "featDat.plot(subplots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Wwnq5A3K-Q"
      },
      "source": [
        "### Preprocessing Data\n",
        "As mentioned, the first step will be to **standardize** the dataset using the mean and standard deviation of the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idFsT3dM3K-Q"
      },
      "outputs": [],
      "source": [
        "dataset = featDat.values\n",
        "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
        "data_std = dataset[:TRAIN_SPLIT].std(axis=0)\n",
        "print(\"data_mean\",data_mean)\n",
        "print(\" data_std\",data_std)\n",
        "\n",
        "dataset = (dataset-data_mean)/data_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW65haQq3K-Q"
      },
      "source": [
        "#### Creating Input and Output Data\n",
        "In a **single step** setup, the model learns to predict a single point in the future based on some history provided.\n",
        "\n",
        "The below function performs the same windowing task as below, however, here it samples the past observation based on the step size given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGKUwF1o3K-Q"
      },
      "outputs": [],
      "source": [
        "def multivariate_data(dataset,      # dataset of input variables\n",
        "                      target,       # dataset of output variables\n",
        "                      start_index,  # first index to use\n",
        "                      end_index,    # end index to use\n",
        "                      history_size,\n",
        "                      target_size,\n",
        "                      step,\n",
        "                      single_step=False):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = range(i-history_size, i, step)\n",
        "        data.append(dataset[indices])\n",
        "\n",
        "        if single_step:\n",
        "            labels.append(target[i+target_size])\n",
        "        else:\n",
        "            labels.append(target[i:i+target_size])\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vodBRkFQ3K-Q"
      },
      "source": [
        "In this tutorial, the network is shown data from the last five (5) days, i.e. 720 observations that are sampled every hour. The sampling is done every one hour since a drastic change is not expected within 60 minutes. Thus, 120 observation represent history of the last five days. For the single step prediction model, the label for a datapoint is the temperature **12 hours into the future**. In order to create a label for this, the temperature after 72(12*6) observations is used."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = []   # list to store description and model results: each model yields a dictionary"
      ],
      "metadata": {
        "id": "AEDKz8EfSmB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNOL-D2h3K-Q"
      },
      "outputs": [],
      "source": [
        "past_history = 120   # use 120 hours as input\n",
        "future_target = 12   # predict value at t+1+future_target ( must be >= 0)\n",
        "past_history = 24    # use 24 hours as input\n",
        "future_target = 0    # predict value at t+1+future_target ( must be >= 0)\n",
        "STEP = 1             # use only elements i, i+STEP, i+2STEP ...\n",
        "ivpred = [name2id['T (degC)']]     # variables to be predicted\n",
        "\n",
        "x_train_single, y_train_single = multivariate_data(dataset,\n",
        "                                                   dataset[:, ivpred],\n",
        "                                                   0,\n",
        "                                                   TRAIN_SPLIT,\n",
        "                                                   past_history,\n",
        "                                                   future_target,\n",
        "                                                   STEP,\n",
        "                                                   single_step=True)\n",
        "x_val_single, y_val_single = multivariate_data(dataset,\n",
        "                                               dataset[:, ivpred],\n",
        "                                               TRAIN_SPLIT,  # first index to use\n",
        "                                               None,         # to end o dataset\n",
        "                                               past_history,\n",
        "                                               future_target,\n",
        "                                               STEP,\n",
        "                                               single_step=True)\n",
        "\n",
        "print ('Single window of past history : {}'.format(x_train_single[0].shape))\n",
        "print('Single input Data',x_train_single[0])\n",
        "print('Single data to be predicted',y_train_single[0])\n",
        "print(\"----- Compare with full data -----\")\n",
        "dataset[:(past_history+STEP+future_target+2),:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tf.data.Dataset for faster execution.\n",
        "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
        "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
        "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
      ],
      "metadata": {
        "id": "4LSYNfqAk2T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdict = {'past_history': past_history, 'future_target' :  future_target,\n",
        "        'STEP': STEP,'ivpred' : ivpred, 'features_considered':features_considered}"
      ],
      "metadata": {
        "id": "7AHTAqIgMK4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the Multilayer Network model"
      ],
      "metadata": {
        "id": "s0FBcpC-LCdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dropout=0.0\n",
        "nhid = [32,32]    # length of hidden vector in each layer\n",
        "patience = 4      # for early stopping\n",
        "\n",
        "mdict.update({\"type\":'MLP', 'dropout':dropout, 'nhid':nhid, 'patience':patience})\n",
        "\n",
        "input_dim = x_train_single.shape\n",
        "nlayer= len(nhid)+1\n",
        "\n",
        "model1 = tf.keras.models.Sequential()\n",
        "\n",
        "# ------ LAYER 1 ----------\n",
        "model1.add(tf.keras.layers.Flatten(input_shape=input_dim[1:])) # convert input matrix to vector x\n",
        "\n",
        "if nlayer>1:\n",
        "  for ilayer in range(nlayer-1):\n",
        "    ### these LSTM layers feed their output to the next LSTM layer\n",
        "    model1.add(tf.keras.layers.Dense(nhid[ilayer],\n",
        "                                     activation = 'relu'))\n",
        "    if dropout>0:\n",
        "      model1.add(tf.keras.Dropout(dropout))\n",
        "\n",
        "### this layer generates the output variable(s) with linear activation\n",
        "model1.add(tf.keras.layers.Dense(len(ivpred)))\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                  patience=patience,\n",
        "                                                  mode='min')\n",
        "\n",
        "model1.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "                          metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
        "                          loss='mae')   # mae mean absolute error, or mse mean square error\n",
        "mdict"
      ],
      "metadata": {
        "id": "Ffb8eijrLSiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EICj7M-nUmbR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVAOF5m3K-Q"
      },
      "source": [
        "### Defining the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q26-NOng3K-Q"
      },
      "outputs": [],
      "source": [
        "dropout=0.0\n",
        "nhid = [32,32]   # length of hidden vector in each layer\n",
        "patience = 3   # for early stopping\n",
        "\n",
        "nlayer = len(nhid)+1\n",
        "\n",
        "mdict.update({\"type\":'LSTM', 'dropout':dropout, 'nhid':nhid, 'patience':patience})\n",
        "\n",
        "model1 = tf.keras.models.Sequential()\n",
        "if nlayer>2:\n",
        "  for ilayer in range(nlayer-1):\n",
        "    ### these LSTM layers feed their output to the next LSTM layer\n",
        "    model1.add(tf.keras.layers.LSTM(32,\n",
        "                                    dropout=dropout,\n",
        "                                    return_sequences=True,\n",
        "                                    input_shape=x_train_single.shape[-2:]))\n",
        "### this LSTM layer just returns the hidden vector at the end\n",
        "model1.add(tf.keras.layers.LSTM(32, dropout=dropout))\n",
        "### this layer generates the output variable(s)\n",
        "model1.add(tf.keras.layers.Dense(len(ivpred)))\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                    patience=patience,\n",
        "                                                    mode='min')\n",
        "\n",
        "model1.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
        "                          metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
        "                          loss='mae')   # mae mean absolute error, or mse mean square error\n",
        "mdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDYg276-3K-Q"
      },
      "source": [
        "### List the Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0836An1GP-ws"
      },
      "source": [
        "Let's check out a sample prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PuSxKRLP-wt"
      },
      "outputs": [],
      "source": [
        "for x, y in val_data_single.take(1):  # take a single element from validation\n",
        "    print(\"shape of a prediction\",model1.predict(x).shape)\n",
        "                                      # initializes parameters\n",
        "model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "pkdyKUbVQaqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit?"
      ],
      "metadata": {
        "id": "QQ-wt5yebDLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR4rIxwR3K-Q"
      },
      "outputs": [],
      "source": [
        "validation_steps = int(len(x_val_single)/BATCH_SIZE)  # number of batches in validation data\n",
        "model1_history = model1.fit(train_data_single,\n",
        "                            epochs=EPOCHS,\n",
        "                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                            validation_data=val_data_single,\n",
        "                            validation_steps=validation_steps,\n",
        "                            callbacks=[early_stopping])\n",
        "mdict.update({'model':model1, 'history':model1_history})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mdict"
      ],
      "metadata": {
        "id": "VrutVFF7YstS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6Na_Mea3K-Q"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(model1_history, 'Single Step Training and validation loss', ymax=-1)"
      ],
      "metadata": {
        "id": "6UW6-ImRp1gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric(model1_history, 'Single Step Training and validation mean_absolute_error',\n",
        "            ymax=-1, metric=\"mean_absolute_error\", ylabel=\"mean absolute error in degree C\")"
      ],
      "metadata": {
        "id": "lzEL4YPKs8mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rcv3P833K-Q"
      },
      "outputs": [],
      "source": [
        "result = model1.evaluate(val_data_single,steps=50)\n",
        "print(model1.metrics_names,result)\n",
        "mdict.update({model1.metrics_names[0]:result[0], model1.metrics_names[1]:result[1]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models.append(mdict)"
      ],
      "metadata": {
        "id": "bUZAZoWAX4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mdict"
      ],
      "metadata": {
        "id": "O10p14eVgDGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "8vZXpJn2b2sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq3FJvf_3K-Q"
      },
      "source": [
        "1 layer LSTM, time series with 1 input:  ['loss'] 0.24583026751875878\n",
        "1 layer LSTM, time series with 3 inputs: ['loss'] 0.2547184965014458\n",
        "2 layer LSTM, time series with 3 inputs: ['loss'] 0.2556210482120514"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "oXK1bqK5Zno5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucNpkSjS3K-Q"
      },
      "source": [
        "#### Predict a single step future\n",
        "Now that the model is trained, let's make a few sample predictions. The model is given the history of three features over the past five days sampled every hour (120 data-points), since the goal is to predict the temperature, the plot only displays the past temperature. The prediction is made one day into the future (hence the gap between the history and prediction)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_time_steps(length):\n",
        "    return list(range(-length, 0))\n",
        "\n",
        "def show_plot(plot_data, delta, title):\n",
        "    labels = ['History', 'True Future', 'Model Prediction']\n",
        "    marker = ['.-', 'rx', 'go']\n",
        "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "    if delta:\n",
        "        future = delta\n",
        "    else:\n",
        "        future = 0\n",
        "\n",
        "    plt.title(title)\n",
        "    for i, x in enumerate(plot_data):\n",
        "        if i:\n",
        "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "               label=labels[i])\n",
        "        else:\n",
        "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "    plt.legend()\n",
        "    plt.xlim([time_steps[0], (future+5)*2])\n",
        "    plt.xlabel('Time-Step')\n",
        "    return plt"
      ],
      "metadata": {
        "id": "6ONv3l5lyM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HTO03f33K-R"
      },
      "outputs": [],
      "source": [
        "for x, y in val_data_single.take(5):  # take these elements from validatio set\n",
        "    pred = model1.predict(x)[0]\n",
        "    inp = x[0]\n",
        "    plot = show_plot([inp[:,ivpred].numpy(),  # lagged values of the output var in the input\n",
        "                      y[0].numpy(),           # observed output value\n",
        "                      pred],                  # predicted value\n",
        "                      future_target,          # how many steps in the future\n",
        "                     'Predicting '+str(future_target)+\" steps from \"+str(past_history)\n",
        "                     + \" past 3-dimensional inputs\" )\n",
        "    plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>**Task 1:**</font>   Investigate a models, where the task is to predict 12 hours into future: `future_target=12`.\n",
        "\n",
        "\n",
        "Try to improve the LSTM model and the MLP model such that the results get better. <br>\n",
        "Modify `features_considered`, number of hidden variables, number of layers, dropout, etc."
      ],
      "metadata": {
        "id": "6BcVOF9qcF8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>**Task 2:**</font>   Modify the code such that several variables can be predicted simultaneously."
      ],
      "metadata": {
        "id": "L14MbHgic3Hk"
      }
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}