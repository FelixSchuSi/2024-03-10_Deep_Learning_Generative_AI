{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T9geBSGH9lz"
      },
      "source": [
        "# Language Modeling with a small GPT-2 Model\n",
        "checked 27.02.24 GPaa√ü\n",
        "\n",
        "This notebook uses code from [https://huggingface.co/gpt2](https://huggingface.co/gpt2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41uzBzFtH9l3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_LeggdHH9l4"
      },
      "source": [
        "The model has 124 million parameters and\n",
        "* embeddings of length 768 for 50257 tokens\n",
        "* embeddings of length 768 for 1024 positions\n",
        "* 12 layers of `GPT2Block`, each with a self-attention `GPT2Attention` and a fully connected network `GPT2MLP`\n",
        "* a final linear model with 768 inputs and 50257 outputs\n",
        "\n",
        "This model was trained on WebText consisting of the text of 45 million outbound links from Reddit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewU9A3bEH9l5"
      },
      "outputs": [],
      "source": [
        "generator.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-XYcFwRH9l5"
      },
      "source": [
        "The generator performs the following actions:\n",
        "* encode the input text as an integer vector using the tokenizer of the model\n",
        "* apply the model function `generate`. You can enter the additional parameters\n",
        "  * `do_sample` (default False) Whether or not to use sampling\n",
        "  * `temperature` (default 1.0) The value used to module the next token probabilities.\n",
        "  * `top_k` (default 50) The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "  * `top_p` (default 1.0) If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
        "        are kept for generation.\n",
        "  * `repetition_penalty` (default 1.0) The parameter for repetition penalty. 1.0 means no penalty.\n",
        "* decode the generated integers to the tokens of the generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXmwFt5EH9l5"
      },
      "outputs": [],
      "source": [
        "generator.model.generate??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glLbdPn7H9l6"
      },
      "outputs": [],
      "source": [
        "### only produce next token with highest prbability\n",
        "res=generator(\"When Donald Trump entered the room,\", max_length=100, num_return_sequences=1, do_sample=False)\n",
        "print(res[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCrtXOzUH9l6"
      },
      "outputs": [],
      "source": [
        "# sample next token according to computed probabilities. Each time a new text is generated.\n",
        "res=generator(\"When Donald Trump entered the room,\", max_length=100, num_return_sequences=3, do_sample=True)\n",
        "for i in range(len(res)):\n",
        "  print(50*'-')\n",
        "  print(res[i]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlMWozeHH9l6"
      },
      "outputs": [],
      "source": [
        "# a next token within the to 90% range of probabilities is selected\n",
        "res=generator(\"When Donald Trump entered the room,\", max_length=100, num_return_sequences=3, do_sample=True, top_p=0.9)\n",
        "for i in range(len(res)):\n",
        "  print(50*'-')\n",
        "  print(res[i]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4iUh7GIH9l7"
      },
      "outputs": [],
      "source": [
        "# the probability estimates are \"flattened\" by the temperature parameter\n",
        "res=generator(\"When Donald Trump entered the room,\", max_length=100, num_return_sequences=3, temperature=3.0)\n",
        "for i in range(len(res)):\n",
        "  print(50*'-')\n",
        "  print(res[i]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytj6HnY3H9l7"
      },
      "outputs": [],
      "source": [
        "start=[\"The black hole started to glow\",\n",
        "       \"When Donald Trump entered the room\",\n",
        "       \"Angela Merkel went to Washington\",\n",
        "       \"Admiral Nelson\"\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0RmQhAcH9l7"
      },
      "outputs": [],
      "source": [
        "for st in start:\n",
        "    res = generator(st, max_length=100, num_return_sequences=3, do_sample=True)\n",
        "    print(\"=\"*100)\n",
        "    for r in res:\n",
        "        print(\"-\"*50)\n",
        "        print(r['generated_text'],\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTNeo 1.3B"
      ],
      "metadata": {
        "id": "juv1GuYhNRXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "kpqVPryDK7ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "\n"
      ],
      "metadata": {
        "id": "w1Mw-_jrKTLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
        "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
        "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
        ")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "SEGaRMXVLL65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recal running on 6 CPUs"
      ],
      "metadata": {
        "id": "6gbq9KaWMqFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=500,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "print(gen_text,\"\\n\")"
      ],
      "metadata": {
        "id": "TDW9QIhoLZWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT-J\n",
        "\n",
        "needs 48GB RAM to load the model"
      ],
      "metadata": {
        "id": "ywmwQCVANai4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()"
      ],
      "metadata": {
        "id": "C3sx8l9NNuME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LMEcg56INwG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"In a shocking finding, scientists discovered a herd of unicorns living in a remote, \"\n",
        "    \"previously unexplored valley, in the Andes Mountains. Even more surprising to the \"\n",
        "    \"researchers was the fact that the unicorns spoke perfect English.\"\n",
        ")\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n"
      ],
      "metadata": {
        "id": "_BmaG8DKN2YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate works only on CPU"
      ],
      "metadata": {
        "id": "jlzbFQOyO0iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=200,\n",
        ")\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "\n",
        "print(gen_text,\"\\n\")"
      ],
      "metadata": {
        "id": "ov1NpE9WOdQR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}