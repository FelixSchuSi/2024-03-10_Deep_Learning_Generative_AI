{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFXnk6RcSp3o"
      },
      "source": [
        "# FastText Word Embeddings  \n",
        "\n",
        "checked 27.02.2024 G.Paa√ü\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Description of FastText\n",
        "\n",
        "[FastText](https://fasttext.cc/) FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.It is implemented in C++. The library was developed by [facebook research](https://research.fb.com/fasttext/).\n",
        "\n"
      ],
      "metadata": {
        "id": "eqrke9zQwJxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "id": "MkyUb01lTukI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "code_folding": [],
        "scrolled": true,
        "id": "kHRev8lvSp3p"
      },
      "outputs": [],
      "source": [
        "import os, sys;\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-X8OlnYSp3r"
      },
      "source": [
        "## Data from Wikipedia\n",
        "We use an excerpt `text8` from the first 109 bytes of the English Wikipedia dump on Mar. 3, 2006. It may be downloaded from [here](http://mattmahoney.net/dc/textdata.html)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip\n",
        "!unzip text8.zip\n",
        "!mv text8 text8.txt"
      ],
      "metadata": {
        "id": "SlxpcZsQVPgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "jHDFlsF9VWkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xlDB3Z-ZSp3r"
      },
      "outputs": [],
      "source": [
        "text8File = 'text8.txt'\n",
        "print(text8File)\n",
        "file = open(text8File, \"r\")\n",
        "words=file.read().split()\n",
        "print('Read wikipedia data. Data size (number of words)', len(words))\n",
        "st = \"\"\n",
        "for w in words[0:1000]:\n",
        "    st += w + \" \"\n",
        "print(\"First words:\\n\" + st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "QL-2M3UDSp3r"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dEoQTcCSp3r"
      },
      "source": [
        "## Training Word Vectors with Word2Vec\n",
        "\n",
        "Enter `FT_wrapper.train?` to get the arguments of the FastText-call:<br/>For word vector learning fasttext has the following **options**:\n",
        "\n",
        "FT_wrapper.train(\n",
        "    ft_path,\n",
        "    corpus_file,\n",
        "    output_file=None,\n",
        "    model='cbow',\n",
        "    size=100,\n",
        "    alpha=0.025,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    word_ngrams=1,\n",
        "    loss='ns',\n",
        "    sample=0.001,\n",
        "    negative=5,\n",
        "    iter=5,\n",
        "    min_n=3,\n",
        "    max_n=6,\n",
        "    sorted_vocab=1,\n",
        "    threads=12,\n",
        "\n",
        "|option     |           meaning |\n",
        "|----------------|----------------------------------|\n",
        "| input             | training file path (required) |\n",
        "|   model     | unsupervised fasttext model {cbow, skipgram} [skipgram]|\n",
        "|    lr                | learning rate [0.05]|\n",
        "|    dim               | size of word vectors [100]|\n",
        "|    ws                | size of the context window [5]|\n",
        "|    epoch             | number of epochs [5]|\n",
        "|    minCount          | minimal number of word occurences [5]|\n",
        "|    minn              | min length of char ngram [3]|\n",
        "|    maxn              | max length of char ngram [6]|\n",
        "|    neg               | number of negatives sampled [5]|\n",
        "|    wordNgrams        | max length of word ngram [1]|\n",
        "|    loss              | loss function {ns, hs, softmax, ova} [ns]|\n",
        "|    bucket            | number of buckets [2000000]|\n",
        "|    thread            | number of threads [number of cpus]|\n",
        "|    lrUpdateRate      | change the rate of updates for the learning rate [100]|\n",
        "|    t                 | sampling threshold [0.0001]|\n",
        "|    verbose           | verbose [2]|\n",
        "\n",
        "### Training without n-grams\n",
        "Learning word vectors on this data can now be achieved with a single command. <br/>See progress on console invoking jupyter notebook.\n",
        "\n",
        "See the progress on the console invoking jupyter notebook.\n",
        "\n",
        "(skipgram Wall time: 16min 40s):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KBh_VekiSp3s"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# train the model without n-grams\n",
        "model = fasttext.train_unsupervised(text8File,         # input text\n",
        "                                    model='skipgram',  # 'cbow', 'skipgram'.\n",
        "                                    dim=100,           # embedding length\n",
        "                                    maxn=0,            # n-grams maximal length\n",
        "                                    ws=5,              # window size\n",
        "                                    thread=10,         # number of threads\n",
        "                                    epoch=5)           # number of epochs\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_model('model0')   # save model"
      ],
      "metadata": {
        "id": "yZJEjsTxnZVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model0=fasttext.load_model('model0')"
      ],
      "metadata": {
        "id": "cmCE5qK2n8LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlap8TyASp3t"
      },
      "source": [
        "### Effect of Training Parameters\n",
        "\n",
        "So far, we run fastText with the default parameters, but depending on the data, these parameters may not be optimal. Let us give an introduction to some of the key parameters for word vectors.\n",
        "\n",
        "The most important parameters of the model are its dimension and the range of size for the subwords.\n",
        "* The dimension (`size`) controls the **size of the vectors**, the larger they are the more information they can capture but requires more data to be learned. But, if they are too large, they are harder and slower to train. By default, we use 100 dimensions, but any value in the 100-300 range is as popular.\n",
        "* The **subwords** are all the substrings contained in a word between the minimum size (`minn`) and the maximal size (`maxn`). By default, we take all the subword between 3 and 6 characters, but other range could be more appropriate to different languages:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW9tJwPjSp3t"
      },
      "source": [
        "Depending on the quantity of data you have, you may want to change the parameters of the training.\n",
        "* The **epoch** parameter controls how many time will loop over your data. By default, we loop over the dataset 5 times. If you dataset is extremely massive, you may want to loop over it less often.\n",
        "* Another important parameter is the **learning rate** (alpha). The higher the learning rate is, the faster the model converge to a solution but at the risk of overfitting to the dataset. The default value is 0.05 which is a good compromise. If you want to play with it we suggest to stay in the range of [0.01, 1].\n",
        "* Finally , fastText is multi-threaded and uses 12 threads by default. If you have less CPU cores (say 4), you can easily set the number of threads using the **threads** flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ydNexuSp3t"
      },
      "source": [
        "### Printing Word Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5VfQWgMMSp3u"
      },
      "outputs": [],
      "source": [
        "print('night\\n',model['night'])\n",
        "print('nights\\n',model['nights'])\n",
        "print('cosine similarity \"night\" \"nights\" = ',scipy.spatial.distance.cosine(model['night'],model['nights']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh-V5Fs_Sp3u"
      },
      "source": [
        "### Nearest neighbor queries\n",
        "\n",
        "A simple way to check the quality of a word vector is to look at its nearest neighbors. This give an intuition of the type of semantic information the vectors are able to capture.\n",
        "\n",
        "This can be achieve with the `nn` functionality. For example, we can query the 20 nearest neighbors of a word by running the following command in a command shell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "RAhVa1v3Sp3v"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(model.get_nearest_neighbors('nights',k=15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qY-SYgzMSp3v"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(model.get_nearest_neighbors('nights',k=15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kq8vVFlUSp3v"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(model.get_nearest_neighbors('proton',k=15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "MzgKZplwSp3v"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(model.get_nearest_neighbors('bank',k=15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foqwO66mSp3w"
      },
      "source": [
        "In order to find nearest neighbors, we need to compute a similarity score between words. Our words are represented by continuous word vectors and we can thus apply simple similarities to them. In particular we use the **cosine** of the angles between two vectors. This similarity is computed for all words in the vocabulary, and the 10 most similar words are shown. Of course, if the word appears in the vocabulary, it will appear on top, with a similarity of 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1taKh1OSp3w"
      },
      "source": [
        "### Word Analogies\n",
        "\n",
        "In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France, what Berlin is to Germany.\n",
        "\n",
        "This can be done with the analogies functionality. It takes a word triplet (like king man woman) and computes\n",
        "$$ diff= emb(king)-emb(man) \\\\\n",
        "   res = diff + emb(woman) $$\n",
        "Subsequently the embeddings of words are selected which are closest to $res$.\n",
        "\n",
        "\n",
        "\n",
        "This can be achieve with the `get_analogies`  functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_analogies('king', 'man', 'woman', k=10)"
      ],
      "metadata": {
        "id": "WZTxN72stSsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_analogies('doctor', 'man', 'woman', k=10)"
      ],
      "metadata": {
        "id": "IyHGVJfJttPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_analogies('berlin', 'germany', 'france', k=10)"
      ],
      "metadata": {
        "id": "W43GYbXttyJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "livereveal": {
      "scroll": "True",
      "start_slideshow_at": "selected",
      "theme": "simplePs",
      "transition": "zoom"
    },
    "nbpresent": {
      "slides": {
        "5d0ab039-9649-438a-9ea7-25c0a54307cf": {
          "id": "5d0ab039-9649-438a-9ea7-25c0a54307cf",
          "layout": "grid",
          "prev": null,
          "regions": {
            "5677b434-c42b-49ec-9c82-b0b6c0f24216": {
              "attrs": {
                "height": 0.4166666666666667,
                "pad": 0.01,
                "width": 0.9166666666666666,
                "x": 0.08333333333333333,
                "y": 0.08333333333333333
              },
              "content": {
                "cell": "42e8a721-8167-498f-bc9f-e81913335295",
                "part": "whole"
              },
              "id": "5677b434-c42b-49ec-9c82-b0b6c0f24216"
            },
            "dacdc962-390c-4b5c-a20b-6a4034c5ca0c": {
              "attrs": {
                "height": 0.4166666666666667,
                "pad": 0.01,
                "width": 0.9166666666666666,
                "x": 0.08333333333333333,
                "y": 0.5833333333333334
              },
              "id": "dacdc962-390c-4b5c-a20b-6a4034c5ca0c"
            }
          },
          "theme": null
        }
      },
      "themes": {
        "default": "68cbdb39-de1e-4cec-acd9-8e7df034b6d5",
        "theme": {
          "68cbdb39-de1e-4cec-acd9-8e7df034b6d5": {
            "id": "68cbdb39-de1e-4cec-acd9-8e7df034b6d5",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  155,
                  177,
                  192
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410"
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 8
              },
              "h2": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "font-family": "Merriweather",
              "font-size": 4
            }
          }
        }
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "259px"
      },
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}