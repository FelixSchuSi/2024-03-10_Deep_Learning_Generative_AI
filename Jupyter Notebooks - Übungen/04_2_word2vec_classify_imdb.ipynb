{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Static Word Embeddings\n",
        "\n",
        "Code from [here](https://www.tensorflow.org/text/guide/word_embeddings).\n",
        "\n",
        "This tutorial contains an introduction to word embeddings. <br>\n",
        "You will train your own word embeddings using a simple Keras model for a **sentiment classification** task, and then visualize them."
      ],
      "metadata": {
        "id": "FIm25-Ns0Y5i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymDFczn80WKg"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download IMDb Datset.\n",
        "\n",
        "There is a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There are two classes: positive reviews and negative reviews."
      ],
      "metadata": {
        "id": "dbrQSUVR_DDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "metadata": {
        "id": "XeYvzdjG0mL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train/` directory has pos and neg folders with movie reviews labelled as positive and negative respectively. You will use reviews from pos and neg folders to train a binary classification model."
      ],
      "metadata": {
        "id": "X03wqXZ8_scq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "# remove extra data\n",
        "#remove_dir = os.path.join(train_dir, 'unsup')\n",
        "#shutil.rmtree(remove_dir)\n",
        "os.listdir(train_dir)"
      ],
      "metadata": {
        "id": "AqYGw8BJ05Fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `train` directory to create both train and validation datasets with a split of 20% for validation."
      ],
      "metadata": {
        "id": "D8uMJBMcAYM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "seed = 123\n",
        "train_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
        "    subset='training', seed=seed)\n",
        "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n",
        "    subset='validation', seed=seed)\n"
      ],
      "metadata": {
        "id": "GATkWDSD1BJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at a few movie reviews and their labels (1: positive, 0: negative) from the train dataset."
      ],
      "metadata": {
        "id": "guoxwR8dApyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(label_batch[i].numpy(), text_batch.numpy()[i])\n"
      ],
      "metadata": {
        "id": "skKCAj0C1Ge6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure the dataset for performance\n",
        "\n",
        "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
        "\n",
        "* `.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
        "\n",
        "* `.prefetch()` overlaps data preprocessing and model execution while training.\n",
        "\n",
        "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
      ],
      "metadata": {
        "id": "uLtwa78OAwhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "j6KE45Ub1LbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Embedding layer\n",
        "\n",
        "Keras makes it easy to use word embeddings. The `Embedding` layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer."
      ],
      "metadata": {
        "id": "0o_GaNhUBNT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed a 1,000 word vocabulary into 5 dimensions.\n",
        "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
      ],
      "metadata": {
        "id": "eLFnZ5781P25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you create an Embedding layer, the weights for the embedding are **randomly initialized** (just like any other layer). During training, they are gradually adjusted via **backpropagation**. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on)."
      ],
      "metadata": {
        "id": "pjXCIBiVBsTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = embedding_layer(tf.constant([1, 2, 3]))  # extract embeddings for words with indices 1, 2, 3\n",
        "print('--- initial embeddings for words with indices 1, 2, 3 ---')\n",
        "result.numpy()"
      ],
      "metadata": {
        "id": "PxsasP941zGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Text Data"
      ],
      "metadata": {
        "id": "zRrlVFWsCPdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)                                    # convert to lower case\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')          # replace break by blank\n",
        "  return tf.strings.regex_replace(stripped_html,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '') # replace punctuation\n",
        "\n",
        "\n",
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Note that the layer uses the custom standardization defined above.\n",
        "# Set maximum_sequence length as all samples are not of the same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,                    # replace rare tokens\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)   # all examples have same length\n",
        "\n",
        "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "text_ds = train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(text_ds)   # Computes a vocabulary of string terms from tokens in a dataset.\n"
      ],
      "metadata": {
        "id": "aaigwJOp16rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt?"
      ],
      "metadata": {
        "id": "UzLmyLGdGOZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the original data and the vectorized data."
      ],
      "metadata": {
        "id": "q8mg8TELFo0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for x, y in train_ds:\n",
        "  print('len(x)',len(x))\n",
        "  print(x)\n",
        "  print('len(y)',len(y))\n",
        "  print(y)\n",
        "  if i>2:\n",
        "    break\n",
        "  i +=1"
      ],
      "metadata": {
        "id": "8U4KZWpTCyKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Classification Model\n",
        "\n",
        "Use the Keras Sequential API to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model.\n",
        "\n",
        "*  The `TextVectorization` layer transforms strings into vocabulary indices. <br> You have already initialized `vectorize_layer` as a TextVectorization layer and built its vocabulary by calling `adapt` on the text data `text_ds`. <br>\n",
        "Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
        "\n",
        "*  The `Embedding` layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are:\n",
        "  * element index in batch\n",
        "  * index of word in sequence\n",
        "  * index inside embedding\n",
        "\n",
        "* The `GlobalAveragePooling1D` layer returns a fixed-length output vector for each example by averaging over the sequence dimension (i.e. average over all word embeddings of a sequence). This allows the model to handle input of variable length, in the simplest way possible.\n",
        "\n",
        "* The fixed-length output vector is piped through a fully-connected (Dense) layer with 16 hidden units.\n",
        "\n",
        "* The last layer is densely connected with a single output node.\n"
      ],
      "metadata": {
        "id": "QoSTO9jZH5Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim=16\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,                                         # vectorization on the fly\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),  # get embedding for each word\n",
        "  GlobalAveragePooling1D(),                                # get the average embeddings of a text\n",
        "  Dense(32, activation='relu'),                            # apply nonlinear layer\n",
        "  Dense(1)                                                 # compute a single output y.\n",
        "                                                           # this is converted to probability loss by the loss function\n",
        "])\n"
      ],
      "metadata": {
        "id": "-ZNBvZes1_iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "H-Cd414s2DKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function `BinaryCrossentropy(from_logits=True)` gets the input value $x$ and the observed class value $z\\in\\{ 0,1\\}$. It computes\n",
        "$$ z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))$$\n",
        "yielding\n",
        "* $-\\log(sigmoid(x))$ for $z=1$ and\n",
        "* $-\\log(1 - sigmoid(x))$ for $ z=0$\n",
        "\n",
        "Note that $sigmoid (x) = \\exp(x)/(1+\\exp(x)$ and always output values in $(0.0,1.0)$."
      ],
      "metadata": {
        "id": "80sKo91UNPbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JPYhnylkNi9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),  # need high probability for correct class\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_t9X7M722Ls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=15,\n",
        "    callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "lmFXwf362Ojn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.7898 for 16 <br>\n",
        "0.8002 for 32"
      ],
      "metadata": {
        "id": "6CwFpZo7Q6jT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuG8ya00Q4QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs_infra: no_execute\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "OJPZKCr02hcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the weights and the vocabulary and save them to files"
      ],
      "metadata": {
        "id": "1fNLLm1jPftq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if index == 0:\n",
        "    continue  # skip 0, it's padding.\n",
        "  vec = weights[index]\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(word + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "!ls"
      ],
      "metadata": {
        "id": "jykQEzfK2sze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read files"
      ],
      "metadata": {
        "id": "MmhEEh3xPw3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')   # Downloads the file to the user's local disk via a browser download action.\n",
        "  files.download('metadata.tsv')\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "mA35jgAW20rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open [embedding projector](http://projector.tensorflow.org/)\n",
        "\n",
        "This can also be done in the [notebook](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin)."
      ],
      "metadata": {
        "id": "7JquI_yq5Q_G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "537URbbT3y-3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}