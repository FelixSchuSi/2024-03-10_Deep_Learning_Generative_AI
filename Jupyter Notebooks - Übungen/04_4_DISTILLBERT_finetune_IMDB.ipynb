{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2794de3f",
      "metadata": {
        "id": "2794de3f"
      },
      "source": [
        "## Finetune BERT-Models on IMDB\n",
        "\n",
        "[code on huggingface](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
        "\n",
        "checked 07.02.2024 GPaaß\n",
        "\n",
        "This code can be executed with PyTorch or (with some minor changes) with TensorFlow\n",
        "\n",
        "Maybe you need to remove all files in .cache\n",
        "\n",
        "The task illustrated in this tutorial is supported by the following model architectures in HUGGINGFACE:\n",
        "\n",
        "ALBERT, BART, BERT, BigBird, BigBird-Pegasus, BioGpt, BLOOM, CamemBERT, CANINE, CodeLlama, ConvBERT, CTRL, Data2VecText, DeBERTa, DeBERTa-v2, DistilBERT, ELECTRA, ERNIE, ErnieM, ESM, Falcon, FlauBERT, FNet, Funnel Transformer, GPT-Sw3, OpenAI GPT-2, GPTBigCode, GPT Neo, GPT NeoX, GPT-J, I-BERT, LayoutLM, LayoutLMv2, LayoutLMv3, LED, LiLT, LLaMA, Longformer, LUKE, MarkupLM, mBART, MEGA, Megatron-BERT, Mistral, Mixtral, MobileBERT, MPNet, MPT, MRA, MT5, MVP, Nezha, Nyströmformer, OpenLlama, OpenAI GPT, OPT, Perceiver, Persimmon, Phi, PLBart, QDQBert, Qwen2, Reformer, RemBERT, RoBERTa, RoBERTa-PreLayerNorm, RoCBert, RoFormer, SqueezeBERT, T5, TAPAS, Transformer-XL, UMT5, XLM, XLM-RoBERTa, XLM-RoBERTa-XL, XLNet, X-MOD, YOSO\n",
        "\n",
        "[Advanced classification on GLUE](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate\n"
      ],
      "metadata": {
        "id": "p2l1-BS1Nk5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb003133-0c40-4629-8b23-57acc38363d6"
      },
      "id": "p2l1-BS1Nk5g",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: dill, responses, multiprocess, accelerate, datasets, evaluate\n",
            "Successfully installed accelerate-0.27.2 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sA42_ehJb5Je"
      },
      "id": "sA42_ehJb5Je"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dff72c3a",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "dff72c3a"
      },
      "outputs": [],
      "source": [
        "# tag: parameters for papermill. View > Cell Toolbar > Tags. Need papermill library\n",
        "#prm = \"small\"              # small: just use 1 epoch\n",
        "#prm = \"full\"              # small: just use 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eeac9c3",
      "metadata": {
        "id": "1eeac9c3"
      },
      "outputs": [],
      "source": [
        "# clear GPU memory\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "QrFdS8wq5G4h"
      },
      "id": "QrFdS8wq5G4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load IMDB Data\n",
        "Start by loading the IMDb dataset from the Datasets library:"
      ],
      "metadata": {
        "id": "DtixZiVVU9J3"
      },
      "id": "DtixZiVVU9J3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b46c96",
      "metadata": {
        "id": "72b46c96"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two fields in this dataset:\n",
        "\n",
        "    * text: the movie review text.\n",
        "    * label: a value that is either 0 for a negative review or 1 for a positive review."
      ],
      "metadata": {
        "id": "Blk-AtIQVst4"
      },
      "id": "Blk-AtIQVst4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14e506a",
      "metadata": {
        "id": "d14e506a"
      },
      "outputs": [],
      "source": [
        "print(\"----- example of a review with negative rating -----\")\n",
        "imdb[\"test\"][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "643d24ef",
      "metadata": {
        "id": "643d24ef"
      },
      "outputs": [],
      "source": [
        "print(\"----- example of a review with positive rating -----\")\n",
        "imdb[\"test\"][20000]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "Select the type of model"
      ],
      "metadata": {
        "id": "s7qu-YHxQJSY"
      },
      "id": "s7qu-YHxQJSY"
    },
    {
      "cell_type": "code",
      "source": [
        "model_type = \"bert-base-cased\"\n",
        "model_type = \"distilbert-base-uncased\"\n"
      ],
      "metadata": {
        "id": "ZBlq8UGkQBko"
      },
      "id": "ZBlq8UGkQBko",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fe6135",
      "metadata": {
        "id": "89fe6135"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a preprocessing function to tokenize text and truncate sequences to be no longer than DistilBERT’s maximum input length:"
      ],
      "metadata": {
        "id": "DWpHFAI6Qi8P"
      },
      "id": "DWpHFAI6Qi8P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec56bb9a",
      "metadata": {
        "id": "ec56bb9a"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"],  truncation=True)\n",
        "tokenized_imdb = imdb.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It’s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
      ],
      "metadata": {
        "id": "OXN_BooCW4s2"
      },
      "id": "OXN_BooCW4s2"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "wKppq0_cXQIL"
      },
      "id": "wKppq0_cXQIL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "hakQNqhUXhm6"
      },
      "id": "hakQNqhUXhm6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create a function that passes your predictions and labels to compute to calculate the accuracy:"
      ],
      "metadata": {
        "id": "UYpwio5sXovM"
      },
      "id": "UYpwio5sXovM"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "rGqGhPd-XsNJ"
      },
      "id": "rGqGhPd-XsNJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "Before you start training your model, create a map of the expected ids to their labels with id2label and label2id:"
      ],
      "metadata": {
        "id": "0hSYDnmAZeWV"
      },
      "id": "0hSYDnmAZeWV"
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ],
      "metadata": {
        "id": "4AP_zkgQZFQ5"
      },
      "id": "4AP_zkgQZFQ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you aren’t familiar with finetuning a model with the Trainer, take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/training#train-with-pytorch-trainer)!\n",
        "\n",
        "\n",
        "Load DistilBERT with AutoModelForSequenceClassification along with the number of expected labels, and the label mappings:"
      ],
      "metadata": {
        "id": "P2bWbWW8ZMxc"
      },
      "id": "P2bWbWW8ZMxc"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_type, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")"
      ],
      "metadata": {
        "id": "4PtJp60PaL2a"
      },
      "id": "4PtJp60PaL2a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, only three steps remain:\n",
        "\n",
        "1.    Define your training hyperparameters in `TrainingArguments`. The only required parameter is `output_dir` which specifies where to save your model.  At the end of each epoch, the `Trainer` will evaluate the accuracy and save the training checkpoint.\n",
        "1.    Pass the training arguments to `Trainer` along with the model, dataset, tokenizer, data collator, and compute_metrics function.\n",
        "1.   Call `train()` to finetune your model."
      ],
      "metadata": {
        "id": "HYEDCiG9aSTv"
      },
      "id": "HYEDCiG9aSTv"
    },
    {
      "cell_type": "code",
      "source": [
        "TrainingArguments?"
      ],
      "metadata": {
        "id": "-6tN16IVgOrC"
      },
      "id": "-6tN16IVgOrC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        ")"
      ],
      "metadata": {
        "id": "7ODkPLbDgFqK"
      },
      "id": "7ODkPLbDgFqK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "aHDsg5MDgYUD"
      },
      "id": "aHDsg5MDgYUD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "About 11:30 minutes with A100 40GB. RAM usage 6.8GB of 40.1 GB. Validation loss: 0.2263, Accuracy: 0.932\n",
        "\n",
        "About 1:30 for evaluations of test data\n",
        "\n",
        "Check GPU activity in terminal with `watch nvidia-smi`\n",
        "\n",
        "| GPU | Val loss | Val acc | execution time |\n",
        "|:--------:|:--------:|:--------:|:--------:|\n",
        "| A100 40GB | 0.2263 | 0.932 | 11:30 |\n",
        "|T4 40GB | 0.2263 | 0.932 | 36:00|"
      ],
      "metadata": {
        "id": "nuQqIIxQ-IPa"
      },
      "id": "nuQqIIxQ-IPa"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "i_v8oiqla5Oz"
      },
      "id": "i_v8oiqla5Oz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "04bc3cb0",
      "metadata": {
        "id": "04bc3cb0"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now we can use the model for inference to classify a new text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Application to a single text with `pipeline`"
      ],
      "metadata": {
        "id": "0elY1eS9GD5M"
      },
      "id": "0elY1eS9GD5M"
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
      ],
      "metadata": {
        "id": "egIgOeTHa2De"
      },
      "id": "egIgOeTHa2De",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was saved to the directory `my_awesome_model`. You can open a terminal (bottom left) and enter the command `ls -l` to see the directory.\n",
        "\n",
        "The simplest way to try out your finetuned model for inference is to use it in a pipeline(). Instantiate a pipeline for sentiment analysis with your model, and pass your text to it:"
      ],
      "metadata": {
        "id": "rNe3wUkiCXFm"
      },
      "id": "rNe3wUkiCXFm"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cwd = os.getcwd()\n",
        "cwd"
      ],
      "metadata": {
        "id": "SKyxUKe-EyOk"
      },
      "id": "SKyxUKe-EyOk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"my_awesome_model/checkpoint-3126\""
      ],
      "metadata": {
        "id": "4waRw1KUGjRu"
      },
      "id": "4waRw1KUGjRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_dir)\n",
        "classifier(text)"
      ],
      "metadata": {
        "id": "PWrw9YopCaHT"
      },
      "id": "PWrw9YopCaHT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate — Runs an evaluation loop and returns metrics."
      ],
      "metadata": {
        "id": "TB8PmQLx-Rxc"
      },
      "id": "TB8PmQLx-Rxc"
    },
    {
      "cell_type": "code",
      "source": [
        "metrik = trainer.evaluate()\n",
        "metrik"
      ],
      "metadata": {
        "id": "y1DR69vK-VSX"
      },
      "id": "y1DR69vK-VSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict — Returns predictions (with metrics if labels are available) on a test set."
      ],
      "metadata": {
        "id": "pjynsgSC9yUC"
      },
      "id": "pjynsgSC9yUC"
    },
    {
      "cell_type": "code",
      "source": [
        "preds = trainer.predict(test_dataset=tokenized_imdb[\"test\"])"
      ],
      "metadata": {
        "id": "wTTjwTrK-BZw"
      },
      "id": "wTTjwTrK-BZw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    mx = np.max(x,axis=1)                 # compute max of rows of x\n",
        "    mx = np.repeat(mx[:,np.newaxis],2,1)  # expand to new dimension\n",
        "    xx3 = x-mx                            # subtract maximum (avoid overflow)\n",
        "    ex = np.exp(xx3)                      # compute exponent\n",
        "    ex_sum = np.sum(ex,axis=1)            # sum of rows\n",
        "    ex_sum = np.repeat(ex_sum[:,np.newaxis],2,1) #\n",
        "    return ex/ex_sum                      # [exp(x_1),...,exp(x_k)]/(exp(x_1)+...+ex"
      ],
      "metadata": {
        "id": "Ir65J49QCkLY"
      },
      "id": "Ir65J49QCkLY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preds.predictions))\n",
        "probs = softmax(preds.predictions)\n",
        "for i in range(10):\n",
        "  print(i,\"prob=\",probs[i,0],\"\\tlabel=\",tokenized_imdb[\"test\"]['label'][i])\n",
        "#print(probs[:10])\n"
      ],
      "metadata": {
        "id": "dgsWMz1GAePG"
      },
      "id": "dgsWMz1GAePG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Execution of `pipeline` Commands\n",
        "\n",
        "Tokenize the text and return PyTorch tensors:"
      ],
      "metadata": {
        "id": "b32C_5HVGOI7"
      },
      "id": "b32C_5HVGOI7"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "2ZU7ATA8GZcf"
      },
      "id": "2ZU7ATA8GZcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as your inputs to the model and return the logits:"
      ],
      "metadata": {
        "id": "vetCWczsG33E"
      },
      "id": "vetCWczsG33E"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "with torch.no_grad():\n",
        "  logits = model(**inputs).logits  # predict output\n",
        "print(\"logits\",logits)"
      ],
      "metadata": {
        "id": "yok4D8kfG6md"
      },
      "id": "yok4D8kfG6md",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the class with the highest probability, and use the model’s id2label mapping to convert it to a text label:"
      ],
      "metadata": {
        "id": "cvMPV7jpHNf9"
      },
      "id": "cvMPV7jpHNf9"
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_id = logits.argmax().item()\n",
        "id2label[predicted_class_id] # model.config.id2label[predicted_class_id]2label"
      ],
      "metadata": {
        "id": "SdW8rYNQHltR"
      },
      "id": "SdW8rYNQHltR",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}