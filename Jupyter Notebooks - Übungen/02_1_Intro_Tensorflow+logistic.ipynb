{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7rKJtvht7y3"
      },
      "source": [
        "# Intro to TensorFlow: Logistic Regression Classifier\n",
        "\n",
        "Checked 25.02.24 GPaa√ü\n",
        "\n",
        "A detailed introduction to tensorflow is [here](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/).\n",
        "\n",
        "First we have to load a number of libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at74TKcot7y5"
      },
      "outputs": [],
      "source": [
        "import os, sys, math\n",
        "import numpy as np                             # library for vector, matrix, tensor operations\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt                # plotting library\n",
        "import pandas as pd                            # data handling library\n",
        "from IPython.display import display, Markdown  # for formatting answers to questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sTLAREot7y6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf                        # main tensorflow library\n",
        "from tensorflow import keras                   # keras on top of tensorflow\n",
        "from keras.models import Sequential, clone_model\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "print(\"python version =\",sys.version_info)      # check the version of python\n",
        "print(\"tensorflow version:\", tf.__version__)    # check the version of tensorflow\n",
        "print(\"Tensorflow compute devices (CPU, GPU): \")\n",
        "for dv in tf.config.list_physical_devices():\n",
        "    print(\"\\t\",dv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF0ebPwut7y6"
      },
      "source": [
        "## Numpy and TensorFlow: Basic Concepts\n",
        "\n",
        "\n",
        "Both numpy and  tensorflow are languages to process n-dimensional arrays.\n",
        "- The values of an array are stored in a contiguous vector.\n",
        "- The dimension **information** is kept separate.\n",
        "- A large library of high-level mathematical functions to operate on these arrays.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H18hWEqt7y7"
      },
      "source": [
        "### Matrix Multiplication  in Numpy\n",
        "Matrix multiplication $C=A * B$ <br>\n",
        "Requirement: number of columns of A == number rows of B\n",
        "\n",
        "Every command is **immediately executed**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KCuBiNMt7y8"
      },
      "outputs": [],
      "source": [
        "v = np.array([1, 2, 3, 4])\n",
        "print(\"v=\" + str(v))\n",
        "B = v.reshape([2, 2])  # reshape as 2x2 - matrix.\n",
        "\n",
        "print(\"B=\\n\" + str(B))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ53kFnut7y9"
      },
      "outputs": [],
      "source": [
        "A = np.matrix([[1.0, 1.0, -1.0], [0.0, 2.0, 3.0]])  # 2x3 matrix\n",
        "print(\"A=\\n\" + str(A))\n",
        "C = np.dot(B, A)  # matrix product: 2x3 matrix\n",
        "print(\"C=\\n\" +\n",
        "      str(C))  # The values are available as soon as operation is executed.\n",
        "print(\"type(A)=\",type(A))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWg6_nZKt7y-"
      },
      "source": [
        "### Matrix Multiplication  and Other Computations in Tensorflow\n",
        "Tensorflow can do arithmetic with **tensors**: vectors, matrices and higher-dimensional arrays\n",
        "  \n",
        "- Tensors are similar to NumPy ndarray objects,\n",
        "- tf.Tensor objects have a data type and a shape. Additionally,\n",
        "- tf.Tensors can reside in accelerator memory (like a GPU).\n",
        "- TensorFlow offers a rich library of operations (tf.add, tf.matmul, tf.linalg.inv etc.) that consume and produce tf.Tensors.\n",
        "\n",
        "These operations automatically convert native Python types, for example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVhxJiqpt7y_"
      },
      "outputs": [],
      "source": [
        "# vector and matrix computations\n",
        "At = tf.constant([[1.0, 1.0, -1.0], [0.0, 2.0, 3.0]]) ;    print(\"At=\\n\",At)\n",
        "vt = tf.constant([1.0, 2.0, 3.0, 4.0], name=\"this_is_vt\"); print(\"vt=\",vt)\n",
        "Bt = tf.reshape(vt, [2, 2], name=\"this_is_Bt\");            print(\"Bt=\\n\",Bt)\n",
        "Ct = tf.matmul(Bt, At);                                    print(\"Ct=\\n\",Ct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2oISpSYt7y_"
      },
      "source": [
        "Same results as with numpy.\n",
        "- `vt, At, Bt`, and `Ct` are tensors\n",
        "- have type, shape,  name (optional), and values.\n",
        "- New nodes are automatically built into the underlying graph!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKreGPZmt7zA"
      },
      "outputs": [],
      "source": [
        "print(tf.add(1.0, 2))                   # add two scalars\n",
        "print(tf.square(5))                     # compute square\n",
        "print(tf.reduce_sum([1, 2, 3]))         # compute a sum. Axis= gives dimension to sum over\n",
        "print(tf.add([1, 2], [3, 4]))           # add two vectors\n",
        "print(tf.add([1, 2], [3, 4]).numpy())   # add two vectors. .numpy() returns the numpy array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmBC9eGct7zA"
      },
      "outputs": [],
      "source": [
        "tf.matmul?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG9etXyjt7zA"
      },
      "source": [
        "Getting information:\n",
        "\n",
        "* get completions:  ` tf.m<TAB> `\n",
        "* get documentation: ` tf.matmul? `\n",
        "* show python code of the function: ` tf.matmul??`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`print_mat`: pretty-print a matrix or dataframe"
      ],
      "metadata": {
        "id": "V1uOVL3BgvjI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W8P46ORt7zB",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def print_mat(x, title=\"\", prtDim=True, max_rows=10, max_columns=10, precision=3, doRound=True,index=None, rowNames=None, colNames=None ):\n",
        "    \"\"\" use pandas display to print a dataframe\n",
        "        title: to be printed\n",
        "        max_rows: number or None\n",
        "        max_columns: number or None\n",
        "        precision: number\n",
        "        doRound: True  perform rounding (avoid E notation)\n",
        "        index: None  row names\n",
        "        columns: None column names\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import numpy as np\n",
        "    with pd.option_context('display.max_rows', max_rows, 'display.max_columns', max_columns, 'display.precision',precision):\n",
        "        # pd.options.display.max_columns = None\n",
        "        if tf.is_tensor(x):\n",
        "            x = x.numpy()\n",
        "        if doRound:\n",
        "            x = np.round(x,decimals=precision)\n",
        "        if title!=\"\":\n",
        "            if prtDim:\n",
        "                print(title,x.shape)\n",
        "            else:\n",
        "                print(title,x.shape)\n",
        "        display(pd.DataFrame(x,index=rowNames, columns=colNames))     # use smaller font\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function resets all random generators to a given state  $\\longrightarrow$ an **identical** stream of random numbers is generated."
      ],
      "metadata": {
        "id": "wM3Q0VSqkUA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random as python_random\n",
        "def reset_seeds(num):\n",
        "  \"\"\" reset random number generators \"\"\"\n",
        "  np.random.seed(num)\n",
        "  python_random.seed(num)\n",
        "  tf.random.set_seed(num+1)"
      ],
      "metadata": {
        "id": "y6wMGrsi5ksh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzm86gYwt7zB"
      },
      "source": [
        "## Logistic Regression for Simple Data\n",
        "We generate two datasets with two variables for visualization.\n",
        "### Generate Data with Two Well-separated Classes\n",
        "First define softmax function and a plot routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMY5FnKet7zB"
      },
      "outputs": [],
      "source": [
        "### define the softmax function\n",
        "### exp(x_1)/(exp(x_1)+...+exp(x_k)) = exp(x_1-mx)/(exp(x_1-mx)+...+exp(x_k-mx))\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    nc=xx_2.shape[1]\n",
        "    mx = np.max(xx_2,axis=1)              # compute max of rows of xx_2\n",
        "    mx = np.repeat(mx[:,np.newaxis],nc,1)  # expand to new dimension\n",
        "    xx3 = xx_2-mx                         # subtract maximum (avoid overflow)\n",
        "    ex = np.exp(xx3)                      # compute exponent\n",
        "    ex_sum = np.sum(ex,axis=1)            # sum of rows\n",
        "    ex_sum = np.repeat(ex_sum[:,np.newaxis],nc,1) #\n",
        "    return ex/ex_sum                      # [exp(x_1),...,exp(x_k)]/(exp(x_1)+...+exp(x_k))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colormap = np.array(['b', 'r', 'g'])\n",
        "colormap[:2]"
      ],
      "metadata": {
        "id": "m1Kojwu2eypJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`plot_points_prob`: function to plot the points of a training set with their class probabilities"
      ],
      "metadata": {
        "id": "nBm-VcLOR1Ns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riVIe-yUt7zC"
      },
      "outputs": [],
      "source": [
        "#   @title\n",
        "def plot_points_prob(model, xx, yy, iclass=1, psize = (6.5,6), useMaxProb=False,\n",
        "                     colormap = np.array(['b', 'r','g'])):\n",
        "  \"\"\" function to plot the points of a training set with their class \"\"\"\n",
        "  ngrid=100\n",
        "  mv=np.ceil(np.max(np.abs(xx))*10)/10\n",
        "  X, Y = np.meshgrid(np.linspace(-mv, mv, ngrid), np.linspace(-mv, mv, ngrid))\n",
        "  #print(X.shape,Y.shape)\n",
        "  xf = X.flatten()\n",
        "  yf = Y.flatten()\n",
        "  X_grid = np.column_stack((xf ,yf))\n",
        "  #print(X_grid.shape)\n",
        "  Y_pred = model.predict(X_grid)\n",
        "  nclass = Y_pred.shape[1]\n",
        "  #print(xf.shape,yf.shape, Y_pred[:,1].shape)\n",
        "\n",
        "  plt.figure(figsize=psize)\n",
        "  if useMaxProb:\n",
        "    plt.title(\"Maximum probability of a class\")\n",
        "    Yp = Y_pred.max(axis=1)\n",
        "  else:\n",
        "    plt.title(\"Probability of class \"+str(iclass))\n",
        "    Yp = Y_pred[:,iclass]\n",
        "\n",
        "  Z = np.reshape(Yp,(ngrid,ngrid))\n",
        "  #print(Z.shape)\n",
        "  plt.xlabel=(\"x1\")\n",
        "  plt.ylabel=(\"x2\")\n",
        "  # RdBu, Spectral, coolwarm\n",
        "  #plt.contour(X, Y, Z, [0.25,0.75], colors='black',linewidths=0.5);\n",
        "  plt.contour(X, Y, Z, [0.5], colors='green',linewidths=0.7);\n",
        "\n",
        "\n",
        "\n",
        "  plt.pcolor(X,Y,Z,vmin = 0.0, vmax = 1.0,cmap='coolwarm')\n",
        "  plt.colorbar()\n",
        "  plt.scatter(xx[:,0],xx[:,1], c=colormap[yy.astype(int)])\n",
        "  #plt.plot(x1g, x2g, 'bo')  # plot x and y using blue circle markers\n",
        "\n",
        "  #plt.plot(x1l, x2l, 'r+')  # plot x and y using blue circle markers\n",
        "\n",
        "  #plt.scatter([0.1,0.2],[0.1,0.4])\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_points_maxprob(model, xx, yy, join=[], psize = (6,6),\n",
        "                       colormap = np.array(['b', 'r', 'g'])):\n",
        "  \"\"\"Plot training data xx, yy and the class with maximal probability\"\"\"\n",
        "  ngrid=100\n",
        "  mv=np.ceil(np.max(np.abs(xx))*10)/10\n",
        "  X, Y = np.meshgrid(np.linspace(-mv, mv, ngrid), np.linspace(-mv, mv, ngrid))\n",
        "  #print(X.shape,Y.shape)\n",
        "  xf = X.flatten()\n",
        "  yf = Y.flatten()\n",
        "  X_grid = np.column_stack((xf ,yf))\n",
        "  #print(X_grid.shape)\n",
        "  Y_pred = model.predict(X_grid)\n",
        "  nclass=Y_pred.shape[1]\n",
        "  y_mx=Y_pred.argmax(axis=1)  # class with maximal probability\n",
        "\n",
        "  ### plot training set\n",
        "  plt.figure(figsize=psize)\n",
        "  plt.title(\"Training Data and  Areas where a Class has Maximum Probability\")\n",
        "  colormap=colormap[:nclass]\n",
        "  plt.scatter(xx[:,0],xx[:,1], c=colormap[yy.astype(int)])\n",
        "\n",
        "\n",
        "  plt.xlabel=(\"x1\")\n",
        "  #plt.ylabel=(\"x2\")\n",
        "  colormap = np.array(['b', 'r', 'g'])\n",
        "  plt.scatter(X.flatten(),Y.flatten(), c=colormap[y_mx.astype(int)],alpha=0.05)\n",
        "\n",
        "  plt.scatter(xx[:,0],xx[:,1], c=colormap[yy.astype(int)])\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "r9r5ikLt6JiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txpP_vzAt7zC"
      },
      "source": [
        "Define a datasets with two blobs corresponding to two classes or thre blobs for three classes.\n",
        "* Every **row** is an example in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Simple Dataset"
      ],
      "metadata": {
        "id": "RJaWLt9TN2dS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkOa-bCOt7zD"
      },
      "outputs": [],
      "source": [
        "def data_2blobs(n, mean1= [-1.0,-0.5], mean2=[1.0,0.5], vr = 0.4):\n",
        "    cov1=[[vr,0.0],[0.0,vr]]\n",
        "    cov2=[[vr,0.0],[0.0,vr]]\n",
        "    n2 = int(math.ceil(n/2))\n",
        "    xx1 = np.random.multivariate_normal(mean1, cov1, size=n2)   # first class\n",
        "    yy1 = np.zeros(n2)\n",
        "    xx2 = np.random.multivariate_normal(mean2, cov2, size=n2)   # second class\n",
        "    yy2 = np.ones(n2)\n",
        "    #print(yy1,yy2,np.concatenate([yy1,yy2]))\n",
        "    return np.concatenate([xx1,xx2]), np.concatenate([yy1,yy2])\n",
        "\n",
        "def data_3blobs(n, mean1= [-1.0,-0.5], mean2=[1.0,0.5], mean3=[1.0,-0.5], vr = 0.4):\n",
        "    cov1=[[vr,0.0],[0.0,vr]]\n",
        "    cov2=[[vr,0.0],[0.0,vr]]\n",
        "    cov3=[[vr,0.0],[0.0,vr]]\n",
        "    n2 = int(math.ceil(n/2))\n",
        "    xx1 = np.random.multivariate_normal(mean1, cov1, size=n2)   # first class\n",
        "    yy1 = np.zeros(n2)\n",
        "    xx2 = np.random.multivariate_normal(mean2, cov2, size=n2)   # second class\n",
        "    yy2 = np.ones(n2)\n",
        "    xx3 = np.random.multivariate_normal(mean3, cov2, size=n2)   # third class\n",
        "    yy3 = np.ones(n2)*2\n",
        "    #print(yy1,yy2,np.concatenate([yy1,yy2]))\n",
        "    return np.concatenate([xx1,xx2,xx3]), np.concatenate([yy1,yy2,yy3])\n",
        "\n",
        "def data_xor(n, overlap=0.0):\n",
        "    n4 = int(math.ceil(n/4))\n",
        "    xx1 = np.column_stack((np.random.uniform(low=0.0-overlap, high=1.0, size=n4),\n",
        "                           np.random.uniform(low=0.0-overlap, high=1.0, size=n4)))\n",
        "    yy1 = np.zeros(n4)\n",
        "    xx2 = np.column_stack((np.random.uniform(low=-1.0, high=0.0+overlap, size=n4),\n",
        "                           np.random.uniform(low=-1.0, high=0.0+overlap, size=n4)))\n",
        "    yy2 = np.zeros(n4)\n",
        "    xx3 = np.column_stack((np.random.uniform(low=-1.0, high=0.0+overlap, size=n4),\n",
        "                           np.random.uniform(low=0.0-overlap, high=1.0, size=n4)))\n",
        "    yy3 = np.ones(n4)\n",
        "    xx4 = np.column_stack((np.random.uniform(low=0.0-overlap, high=1.0, size=n4),\n",
        "                           np.random.uniform(low=-1.0, high=0.0+overlap, size=n4)))\n",
        "    yy4 = np.ones(n4)\n",
        "    return np.concatenate([xx1,xx2,xx3,xx4]), np.concatenate([yy1,yy2,yy3,yy4])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# always yields the same data and parameters\n",
        "reset_seeds(4241)   ## reproducible random parameters"
      ],
      "metadata": {
        "id": "pJZCD3dN6IIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFA8wK3at7zD"
      },
      "outputs": [],
      "source": [
        "use_data=\"3blobs\"  # 2blobs, 3blobs, or xor\n",
        "n_obs=16\n",
        "if use_data ==\"2blobs\":\n",
        "    nclass=2\n",
        "    xx, yy = data_2blobs(n_obs)    # training data\n",
        "    xx_val, yy_val = data_2blobs(n_obs)        # validation data\n",
        "if use_data ==\"3blobs\":\n",
        "    nclass=3\n",
        "    xx, yy = data_3blobs(n_obs)    # training data\n",
        "    xx_val, yy_val = data_3blobs(n_obs)        # validation data\n",
        "if use_data ==\"xor\":\n",
        "    nclass=2\n",
        "    xx, yy = data_xor(n_obs)    # training data\n",
        "    xx_val, yy_val = data_xor(n_obs)        # validation data\n",
        "print_mat(xx,\"xx\")\n",
        "print_mat(yy,\"yy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training and test data."
      ],
      "metadata": {
        "id": "tUuqDsmhSEaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgtSFkwNt7zE"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Plot the training and test data\n",
        "ig, ax = plt.subplots(1, 2,figsize=(8,3.3))\n",
        "colormap = np.array(['b', 'r', 'g'])[:nclass]\n",
        "ax[0].title.set_text('Training Data')\n",
        "ax[0].scatter(xx[:,0],xx[:,1], c=colormap[yy.astype(int)])\n",
        "ax[1].title.set_text('Test Data')\n",
        "ax[1].scatter(xx_val[:,0],xx_val[:,1], c=colormap[yy_val.astype(int)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQmG2mSt7zE"
      },
      "source": [
        "### Define the logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds(42)   ## reproducible random parameters"
      ],
      "metadata": {
        "id": "61Uwqjyu4q3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dense(units, activation = f)` generates a layer $ y = f(x*A+b)$. <br>\n",
        "We use the `softmax` function as activation function $f$."
      ],
      "metadata": {
        "id": "q1nSgfmsOnAL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7YykuCit7zE"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "modela = Sequential([            # list of layers. Here is only one layer.\n",
        "  Dense(nclass,activation='softmax')  # function: out = softmax(x*A2 +b2)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dense?"
      ],
      "metadata": {
        "id": "RAvSsMM7OuQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvkB22Urt7zE"
      },
      "source": [
        "`modela` is a function. We can **apply** the model to the imput data and get the output probabilities.\n",
        "\n",
        "When the model gets its first inputs, its gets their dimension and **randomly generates the parameters**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQkOBJFot7zE"
      },
      "outputs": [],
      "source": [
        "# compute the prediction for the input xx by the model\n",
        "yprob=modela(xx)      # yprob = softmax(xx*A2 +b2)\n",
        "                      # model initializes its parameters\n",
        "# generate (n x 2) probability matrix for 2 classes\n",
        "print_mat(yprob,\"yprob = softmax(xx*A2 +b2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extract the parameters A and b"
      ],
      "metadata": {
        "id": "cchttTfRP3L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A=modela.layers[0].weights[0].numpy()             # extract A-matrix\n",
        "print(\"----- PARAMETERS -----\")\n",
        "print_mat(A,\"A\")\n",
        "b=modela.layers[0].weights[1].numpy()             # extract b-vector\n",
        "print_mat(b,\"b\")"
      ],
      "metadata": {
        "id": "b2OqT-wSPzG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcpuwzRNt7zF"
      },
      "source": [
        "#### **Repeat Computations with numpy**\n",
        "We repeat the computations by numpy to show what `modela` computes.\n",
        "* The difference is close to 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-BhK5GGt7zF"
      },
      "outputs": [],
      "source": [
        "print(\"----- INPUT -----\")\n",
        "print_mat(xx,\"xx\")\n",
        "print(\"----- NUMPY COMPUTATIONS -----\")\n",
        "xx_1 = np.dot(xx, A)                        # xx * A\n",
        "print_mat(xx_1,\"xx*A\")\n",
        "xx_2 = xx_1 + b                             # xx*A + b\n",
        "print_mat(xx_2,\"xx*A + b\")\n",
        "prb = softmax(xx_2)                         # softmax(xx*A + b)\n",
        "print_mat(prb,\"prb = softmax(xx*A + b)\")\n",
        "print(\"----- DIFFERENCE NUMPY - KERAS  -----\")\n",
        "print(\"difference prb-yprob =\",np.max(prb-yprob))   # difference to value computed by the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Plot the Data and the predicted probability for the untrained model**\n",
        "We plot the data and model prediction:\n",
        "* The dot indicate the observed points. Their color indicates the class red/blue.\n",
        "* The model predicts a probability of red/blue.\n",
        "* The probability for each position in the grid is printed by a color between red and blue. The scale on the right indicates the probability of red.\n",
        "* The lines (coutour lines) indicate the levels of identical probability.\n",
        "\n",
        "The model is **not able** to predict the data correctly."
      ],
      "metadata": {
        "id": "gDUvLGI_WAbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_points_prob(modela, xx, yy, iclass=1)"
      ],
      "metadata": {
        "id": "elvTinWh5ZVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_points_prob(modela, xx, yy, useMaxProb=True)"
      ],
      "metadata": {
        "id": "mzb18lj25uxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plot_points_maxprob(modela, xx, yy, join=[], psize = (6,6),\n",
        "                       colormap = np.array(['b', 'r', 'g']))"
      ],
      "metadata": {
        "id": "t6nflMcsZ2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gN6_5jrt7zG"
      },
      "source": [
        "### Train the Logistic RegressionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2C3jay9t7zG"
      },
      "outputs": [],
      "source": [
        "modela.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr9ZwQNnt7zG"
      },
      "source": [
        "Define a **loss function** for the training set $(x_1,y_1),\\ldots,(x_n,y_n)$\n",
        "$$ loss(w) =  -\\log p(y_1|x_1,w)-\\ldots- \\log p(y_n|x_n,w)$$\n",
        "where $p(y_i|x_i)$ is the probability of class $y_i$ computed for input $x_i$ with the current parameters $w$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"10\" color='red'>**?**</font> 02-c-01  loss function"
      ],
      "metadata": {
        "id": "_NLYXIlUzWID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\n",
        "   rf\"\"\"\n",
        "First question:\n",
        "* $p(y_i|x_i)$ for the $i$-th pair $(x_i,y_i)$ of the training set \\\\\n",
        "  is the probability of the observed class $y_i$  \\\\\n",
        "  computed for input $x_i$ with the current parameters $w$.\n",
        "\n",
        "* $p(y_1|x_1, w) * ... * p(y_n|x_n, w)$ is the probability\n",
        "  of the whole training set, a very small number. \\\\\n",
        "  It measure how well the parameter is compatible with the observed data.\n",
        "* $loss(w)=-\\log p(y_1|x_1, w) - ... - \\log p(y_n|x_n, w)$ is the negative log\n",
        "  of the probability of the whole training set\n",
        "\n",
        "Second question:\n",
        "* The minimum $w^*$ of the loss function $loss(w)$ is identical to the maximum\n",
        "  of the probability of the whole training set.\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "h87173vGOB4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Nr5sCkFt7zG"
      },
      "outputs": [],
      "source": [
        "loss_fn = SparseCategoricalCrossentropy(from_logits=False)  # -negative log-probability of observed x_i\n",
        "\n",
        "print(\"value of the loss\",loss_fn(yy, yprob).numpy()) # loss_fn can be applied to observation\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.04)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <font color='red'>**Task 1:**</font>   Assume have a training set of 60000 elements with 10 classes\n",
        "* What will be the initial value of the loss function?\"\n",
        "* What would be the initial value of the loss function for 100 classes?\"\n",
        "* What is the maximal value of the loss function?\n"
      ],
      "metadata": {
        "id": "stg6J8hF13f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ans=\"\"\" \"\"\""
      ],
      "metadata": {
        "id": "xmk2ZMEm11IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run next cell for an answer"
      ],
      "metadata": {
        "id": "9NEPv7Z3SpON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\n",
        "   rf\"\"\"\n",
        "$loss(w)$ is the negative logarithm of the probability of the whole training set of $n=60000$\n",
        "  $$loss(w) = -\\log(p(y_1|x_1, w)) - ... - \\log(p(y_n|x_n, w))$$\n",
        "  As $\\log(0.1) \\approx {round(math.log(0.1),3)}$ the initial loss value for 10 classes will be {round(60000*math.log(0.1),3)}\n",
        "\n",
        "  As $\\log(0.01) \\approx {round(math.log(0.01),3)}$ the initial loss value for 100 classes will be around {round(60000*math.log(0.01),3)}.\n",
        "\n",
        "  As $\\log(1.0)={round(math.log(1.0),3)}$ the maximal value of the loss function is 0.0\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "5sAidKvgR2Pk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFuMBhHVt7zG"
      },
      "source": [
        "### For Illustration: Apply Gradient Descent Step by Step\n",
        "\n",
        "This code illustrates the inner working of the gradient descent. Usually these details are hidden from the user.\n",
        "\n",
        "* The input has dimensions $dim(xx)=(n,2)$\n",
        "* The output has dimensions $dim(yy)=(n,1)$\n",
        "* The parameter $w=(A,b)$ has length 6 and was  initialized randomly.\n",
        "\n",
        "This code performs a loop:\n",
        "\n",
        "1. Forward propagation  $\\qquad$ $prbs = \\text{softmax}(xx*A + b)$\n",
        "1. Compute loss   $\\qquad$  $\\qquad$  $loss = \\text{loss}\\_\\text{fn}(yy, prbs) = -log(prbs_{yy})$\n",
        "1. Compute the gradient  $\\qquad$  $\\frac{\\partial \\text{loss}}{\\partial w}$\n",
        "1. Update parameters  $\\qquad$  $w := w - \\lambda \\frac{\\partial \\text{loss}}{\\partial w}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7D_CE4Gt7zI"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "epochs = 250\n",
        "nprint = 5\n",
        "models = []\n",
        "loss_arr = []\n",
        "plot_int = 15\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n------------------ Start of epoch {epoch} ------------------\")\n",
        "    if epoch%plot_int==0:\n",
        "      models.append(copy.deepcopy(modela))  # deep copy of the model\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # --- FORWARD PROPAGATION ---\n",
        "        # The transformation of inputs will be recorded on the GradientTape.\n",
        "        prbs = modela(xx, training=True)  # = softmax(xx*A+b)\n",
        "        if epoch<nprint:\n",
        "            print(\"xx\",xx)\n",
        "            print(\"prbs=softmax(xx*A+b)\",prbs.numpy())\n",
        "            print(\"yy\",yy)\n",
        "\n",
        "        # --- COMPUTE LOSS ---\n",
        "        loss_value = loss_fn(yy, prbs)\n",
        "        print(\"loss_value\",loss_value.numpy())\n",
        "        loss_arr.append(loss_value.numpy())\n",
        "    # -------------- COMPUTE GRADIENTS ---------------\n",
        "    # Use the gradient tape to automatically retrieve\n",
        "    # the gradients of the trainable variables with respect to the loss.\n",
        "    # --- Compute Gradients ---\n",
        "    grads = tape.gradient(loss_value, modela.trainable_weights)\n",
        "\n",
        "    if epoch<nprint:\n",
        "        for zi in zip(grads, modela.trainable_weights):\n",
        "            print(\"grad=\\n\",zi[0].numpy(),\"\\nweight=\\n\", zi[1].numpy())\n",
        "\n",
        "    # --- UPDATE PARAMETERS ---\n",
        "    # Run one step of gradient descent by updating\n",
        "    # the value of the variables to minimize the loss.\n",
        "    optimizer.apply_gradients(zip(grads, modela.weights))\n",
        "    if epoch<nprint:\n",
        "        for ww in modela.weights:\n",
        "            print(\"updated weight\",ww.numpy())\n",
        "\n",
        "models.append(copy.deepcopy(modela))  # deep copy of the final model\n",
        "loss_arr.append(loss_value.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwhn8XNft7zI"
      },
      "outputs": [],
      "source": [
        "print(\"--------- weights at start --------------\")\n",
        "print(\"models[0].weights\",models[0].weights)\n",
        "print(\"--------- final weights  --------------\")\n",
        "print(\"modela.weights\",models[1].weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colormap = np.array(['r', 'g'])\n",
        "plt.title('Loss')\n",
        "plt.ylim(0.0,max(loss_arr))\n",
        "plt.plot(loss_arr, label='train loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arxP85BQkNz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_points_prob(modela, xx, yy, useMaxProb=True)"
      ],
      "metadata": {
        "id": "_ba6w_t5oqHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_points_maxprob(modela, xx, yy, join=[], psize = (6,6),\n",
        "                       colormap = np.array(['b', 'r', 'g']))"
      ],
      "metadata": {
        "id": "DVvv6INIyXtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This series of plots shows how the probabilities are trained."
      ],
      "metadata": {
        "id": "SCMzB6hM3QC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(models)):\n",
        "  print(\"Epoch \"+str(i*plot_int)+\" Loss=\"+str(loss_arr[i]))\n",
        "  #plot_points_prob(models[i], xx, yy)\n",
        "  plot_points_maxprob(models[i], xx, yy, join=[], psize = (3,3),\n",
        "                       colormap = np.array(['b', 'r', 'g']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJ-Y3QHCaEoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end the **separating hyperplane** (green) is able to separate the points of both classes without error."
      ],
      "metadata": {
        "id": "k7W5-tseT0D2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGjTP7g_t7zI"
      },
      "source": [
        "### Train Model with Keras\n",
        "Reset the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seeds(42)   ## reproducible random parameters\n",
        "nclass = max(yy)+1\n",
        "print(\"nclass=\", nclass)"
      ],
      "metadata": {
        "id": "WRG-zMPboOG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHLEJgpQt7zJ"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "modelb = Sequential([            # list of layers. Here is only one layer.\n",
        "  Dense(nclass, activation='softmax')  # function: out = softmax(x*A2 +b2)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1zZzTkKt7zJ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.5)   # compatibility problem\n",
        "modelb.compile(optimizer = optimizer,     # optimization method as string or optimizer object. alternative is sgd\n",
        "               loss = loss_fn,         # loss function\n",
        "               metrics = ['accuracy']) # for accuracy computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn9WqZ4st7zJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "t0 = time.time()\n",
        "historyb=modelb.fit(xx,                            # training set input\n",
        "                    yy,                            # training set output\n",
        "                    batch_size=xx.shape[0],        # number of training instances for optimization: all\n",
        "                    validation_data=(xx_val, yy_val),    # validation set (optional)\n",
        "                    epochs=50,                          # number of passes through data\n",
        "                    verbose=2)                           # amount of output: 0-2\n",
        "print(\"used {0:.1f} sec\".format(time.time()-t0))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#modelb.history.history\n",
        "performance=modelb.evaluate(xx_val,  yy_val, verbose=2)"
      ],
      "metadata": {
        "id": "_rvoHnH_rc7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history\n"
      ],
      "metadata": {
        "id": "te9J8Rp6ykxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfAivZWtt7zJ"
      },
      "outputs": [],
      "source": [
        "b#@title Plot loss and accuracy\n",
        "def plot_hist(hist):\n",
        "    fig, ax = plt.subplots(1, 2,figsize=(8,3.3))\n",
        "    colormap = np.array(['r', 'g'])\n",
        "    ax[0].title.set_text('Loss')\n",
        "    ax[0].plot(hist['loss'], label='train loss')\n",
        "    ax[0].plot(hist['val_loss'], label='validation loss')\n",
        "    ax[0].set_ylim([0, max(max(hist['loss']), max(hist['val_loss']))])\n",
        "    #ax[0].scatter(xx[:,0],xx[:,1], c=colormap[yy.astype(int)])\n",
        "    ax[0].legend()\n",
        "    ax[0].set_xlabel('epoch')\n",
        "    #ax[0].set_ylabel('loss')\n",
        "    ax[1].title.set_text('Accuracy')\n",
        "    ax[1].plot(hist['accuracy'], label='train accuracy')\n",
        "    ax[1].plot(hist['val_accuracy'], label='validation accuracy')\n",
        "    ax[1].set_ylim([min(min(hist['accuracy']), min(hist['val_accuracy'])),1.0])\n",
        "    ax[1].legend()\n",
        "    ax[1].set_xlabel('epoch')\n",
        "#ax[1].set_ylabel('accuracy')\n",
        "plot_hist(historyb.history)\n",
        "#plot_points_prob(modelb,xx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_points_prob(modelb, xx, yy, useMaxProb=True)\n",
        "plot_points_maxprob(modelb, xx, yy, join=[], psize = (6,6),\n",
        "                       colormap = np.array(['b', 'r', 'g']))"
      ],
      "metadata": {
        "id": "Eh-_7oEerTcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\n",
        "   rf\"\"\"\n",
        "$loss(w)$ is the negative logarithm of the probability of the whole training set.\n",
        "  $$loss(w) = -\\log(p(y_1|x_1, w)) - ... - \\log(p(y_n|x_n, w))$$\n",
        "  As $\\log(0.1) \\approx -2.3$ the initial loss value for 10 classes will be around -23000.0\n",
        "\n",
        "  As $\\log(0.01) \\approx -4.3$ the initial loss value for 100 classes will be around -46000.0\n",
        "\"\"\"))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7sNshgew1yVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "livereveal": {
      "scroll": "True",
      "start_slideshow_at": "selected",
      "theme": "simplePs",
      "transition": "zoom"
    },
    "nbpresent": {
      "slides": {
        "5d0ab039-9649-438a-9ea7-25c0a54307cf": {
          "id": "5d0ab039-9649-438a-9ea7-25c0a54307cf",
          "layout": "grid",
          "prev": null,
          "regions": {
            "5677b434-c42b-49ec-9c82-b0b6c0f24216": {
              "attrs": {
                "height": 0.4166666666666667,
                "pad": 0.01,
                "width": 0.9166666666666666,
                "x": 0.08333333333333333,
                "y": 0.08333333333333333
              },
              "content": {
                "cell": "42e8a721-8167-498f-bc9f-e81913335295",
                "part": "whole"
              },
              "id": "5677b434-c42b-49ec-9c82-b0b6c0f24216"
            },
            "dacdc962-390c-4b5c-a20b-6a4034c5ca0c": {
              "attrs": {
                "height": 0.4166666666666667,
                "pad": 0.01,
                "width": 0.9166666666666666,
                "x": 0.08333333333333333,
                "y": 0.5833333333333334
              },
              "id": "dacdc962-390c-4b5c-a20b-6a4034c5ca0c"
            }
          },
          "theme": null
        }
      },
      "themes": {
        "default": "68cbdb39-de1e-4cec-acd9-8e7df034b6d5",
        "theme": {
          "68cbdb39-de1e-4cec-acd9-8e7df034b6d5": {
            "id": "68cbdb39-de1e-4cec-acd9-8e7df034b6d5",
            "palette": {
              "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
                "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
                "rgb": [
                  252,
                  252,
                  252
                ]
              },
              "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
                "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
                "rgb": [
                  68,
                  68,
                  68
                ]
              },
              "50f92c45-a630-455b-aec3-788680ec7410": {
                "id": "50f92c45-a630-455b-aec3-788680ec7410",
                "rgb": [
                  155,
                  177,
                  192
                ]
              },
              "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
                "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "rgb": [
                  43,
                  126,
                  184
                ]
              },
              "efa7f048-9acb-414c-8b04-a26811511a21": {
                "id": "efa7f048-9acb-414c-8b04-a26811511a21",
                "rgb": [
                  25.118061674008803,
                  73.60176211453744,
                  107.4819383259912
                ]
              }
            },
            "rules": {
              "blockquote": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410"
              },
              "code": {
                "font-family": "Anonymous Pro"
              },
              "h1": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 8
              },
              "h2": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 6
              },
              "h3": {
                "color": "50f92c45-a630-455b-aec3-788680ec7410",
                "font-family": "Lato",
                "font-size": 5.5
              },
              "h4": {
                "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
                "font-family": "Lato",
                "font-size": 5
              },
              "h5": {
                "font-family": "Lato"
              },
              "h6": {
                "font-family": "Lato"
              },
              "h7": {
                "font-family": "Lato"
              },
              "pre": {
                "font-family": "Anonymous Pro",
                "font-size": 4
              }
            },
            "text-base": {
              "font-family": "Merriweather",
              "font-size": 4
            }
          }
        }
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "887px",
        "left": "0px",
        "right": "1548px",
        "top": "111px",
        "width": "212px"
      },
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}